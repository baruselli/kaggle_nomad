{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 600, 3000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns={'spacegroup' : 'sg',\n",
    "                            'number_of_total_atoms' : 'Natoms',\n",
    "                            'percent_atom_al' : 'x_Al',\n",
    "                            'percent_atom_ga' : 'x_Ga',\n",
    "                            'percent_atom_in' : 'x_In',\n",
    "                            'lattice_vector_1_ang' : 'a',\n",
    "                            'lattice_vector_2_ang' : 'b',\n",
    "                            'lattice_vector_3_ang' : 'c',\n",
    "                            'lattice_angle_alpha_degree' : 'alpha',\n",
    "                            'lattice_angle_beta_degree' : 'beta',\n",
    "                            'lattice_angle_gamma_degree' : 'gamma',\n",
    "                            'formation_energy_ev_natom' : 'E',\n",
    "                            'bandgap_energy_ev' : 'Eg'}\n",
    "    \n",
    "    \n",
    "df_train = pd.read_csv(\"./input/train.csv\").rename(columns=columns)\n",
    "df_train[\"dataset\"] = \"train\"\n",
    "df_train[\"E\"]=np.log1p(df_train[\"E\"])\n",
    "df_train[\"Eg\"]=np.log1p(df_train[\"Eg\"])\n",
    "df_test = pd.read_csv(\"./input/test.csv\").rename(columns=columns)\n",
    "df_test[\"dataset\"] = \"test\"\n",
    "df_total = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "len(df_train),len(df_test),len(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065788</td>\n",
       "      <td>1.490362</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.9523</td>\n",
       "      <td>90.0026</td>\n",
       "      <td>8.5513</td>\n",
       "      <td>90.0023</td>\n",
       "      <td>9.1775</td>\n",
       "      <td>train</td>\n",
       "      <td>90.0017</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222343</td>\n",
       "      <td>1.366347</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.1840</td>\n",
       "      <td>90.0186</td>\n",
       "      <td>6.1838</td>\n",
       "      <td>89.9980</td>\n",
       "      <td>23.6287</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0025</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.167293</td>\n",
       "      <td>1.320101</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.7510</td>\n",
       "      <td>90.9688</td>\n",
       "      <td>5.6595</td>\n",
       "      <td>91.1228</td>\n",
       "      <td>13.9630</td>\n",
       "      <td>train</td>\n",
       "      <td>30.5185</td>\n",
       "      <td>3</td>\n",
       "      <td>227</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196553</td>\n",
       "      <td>1.469992</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0036</td>\n",
       "      <td>89.9888</td>\n",
       "      <td>5.0034</td>\n",
       "      <td>90.0119</td>\n",
       "      <td>13.5318</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0017</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049266</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.6614</td>\n",
       "      <td>89.9960</td>\n",
       "      <td>6.6612</td>\n",
       "      <td>90.0006</td>\n",
       "      <td>24.5813</td>\n",
       "      <td>train</td>\n",
       "      <td>119.9893</td>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          E        Eg  Natoms       a    alpha       b     beta        c  \\\n",
       "0  0.065788  1.490362    80.0  9.9523  90.0026  8.5513  90.0023   9.1775   \n",
       "1  0.222343  1.366347    80.0  6.1840  90.0186  6.1838  89.9980  23.6287   \n",
       "2  0.167293  1.320101    40.0  9.7510  90.9688  5.6595  91.1228  13.9630   \n",
       "3  0.196553  1.469992    30.0  5.0036  89.9888  5.0034  90.0119  13.5318   \n",
       "4  0.049266  0.866806    80.0  6.6614  89.9960  6.6612  90.0006  24.5813   \n",
       "\n",
       "  dataset     gamma  id   sg    x_Al    x_Ga   x_In  \n",
       "0   train   90.0017   1   33  0.6250  0.3750  0.000  \n",
       "1   train  120.0025   2  194  0.6250  0.3750  0.000  \n",
       "2   train   30.5185   3  227  0.8125  0.1875  0.000  \n",
       "3   train  120.0017   4  167  0.7500  0.0000  0.250  \n",
       "4   train  119.9893   5  194  0.0000  0.6250  0.375  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>24.8145</td>\n",
       "      <td>90.0002</td>\n",
       "      <td>6.3964</td>\n",
       "      <td>104.7733</td>\n",
       "      <td>6.2933</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0001</td>\n",
       "      <td>596</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>0.4062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.5783</td>\n",
       "      <td>90.0008</td>\n",
       "      <td>9.4849</td>\n",
       "      <td>89.9967</td>\n",
       "      <td>10.1107</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0004</td>\n",
       "      <td>597</td>\n",
       "      <td>33</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.9377</td>\n",
       "      <td>90.0072</td>\n",
       "      <td>6.9372</td>\n",
       "      <td>89.9880</td>\n",
       "      <td>25.0641</td>\n",
       "      <td>test</td>\n",
       "      <td>119.9857</td>\n",
       "      <td>598</td>\n",
       "      <td>194</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.1841</td>\n",
       "      <td>90.0041</td>\n",
       "      <td>8.8659</td>\n",
       "      <td>90.0009</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0007</td>\n",
       "      <td>599</td>\n",
       "      <td>33</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>90.0029</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>90.0031</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>test</td>\n",
       "      <td>89.9969</td>\n",
       "      <td>600</td>\n",
       "      <td>206</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.2812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E  Eg  Natoms        a    alpha       b      beta        c dataset  \\\n",
       "2995 NaN NaN    80.0  24.8145  90.0002  6.3964  104.7733   6.2933    test   \n",
       "2996 NaN NaN    40.0   5.5783  90.0008  9.4849   89.9967  10.1107    test   \n",
       "2997 NaN NaN    80.0   6.9377  90.0072  6.9372   89.9880  25.0641    test   \n",
       "2998 NaN NaN    40.0   5.1841  90.0041  8.8659   90.0009   9.4956    test   \n",
       "2999 NaN NaN    80.0   9.4959  90.0029  9.4956   90.0031   9.4956    test   \n",
       "\n",
       "         gamma   id   sg   x_Al    x_Ga    x_In  \n",
       "2995   90.0001  596   12  0.000  0.5938  0.4062  \n",
       "2996   90.0004  597   33  0.125  0.0000  0.8750  \n",
       "2997  119.9857  598  194  0.000  0.2500  0.7500  \n",
       "2998   90.0007  599   33  0.625  0.0000  0.3750  \n",
       "2999   89.9969  600  206  0.375  0.3438  0.2812  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/cbartel/random-forest-using-elemental-properties\n",
    "def get_vol(a, b, c, alpha, beta, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a (float) - lattice vector 1\n",
    "        b (float) - lattice vector 2\n",
    "        c (float) - lattice vector 3\n",
    "        alpha (float) - lattice angle 1 [radians]\n",
    "        beta (float) - lattice angle 2 [radians]\n",
    "        gamma (float) - lattice angle 3 [radians]\n",
    "    Returns:\n",
    "        volume (float) of the parallelepiped unit cell\n",
    "    \"\"\"\n",
    "    alpha=alpha*np.pi/180\n",
    "    beta=beta*np.pi/180\n",
    "    gamma=gamma*np.pi/180\n",
    "    return a*b*c*np.sqrt(1 + 2*np.cos(alpha)*np.cos(beta)*np.cos(gamma)\n",
    "                           - np.cos(alpha)**2\n",
    "                           - np.cos(beta)**2\n",
    "                           - np.cos(gamma)**2)\n",
    "\n",
    "\n",
    "    \n",
    "# compute the cell volumes \n",
    "df_total['vol'] = get_vol(df_total['a'], df_total['b'], df_total['c'],\n",
    "                          df_total['alpha'], df_total['beta'], df_total['gamma'])\n",
    "#df_total[['a','b','c','alpha','beta','gamma','vol']].head()\n",
    "df_total['density']=df_total['Natoms']/df_total[\"vol\"]\n",
    "df_total['density_Al']=df_total['density']*df_total['x_Al']\n",
    "df_total['density_Ga']=df_total['density']*df_total['x_Ga']\n",
    "df_total['density_In']=df_total['density']*df_total['x_In']\n",
    "df_total['sg']=df_total['sg'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "      <th>vol</th>\n",
       "      <th>density</th>\n",
       "      <th>density_Al</th>\n",
       "      <th>density_Ga</th>\n",
       "      <th>density_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065788</td>\n",
       "      <td>1.490362</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.9523</td>\n",
       "      <td>90.0026</td>\n",
       "      <td>8.5513</td>\n",
       "      <td>90.0023</td>\n",
       "      <td>9.1775</td>\n",
       "      <td>train</td>\n",
       "      <td>90.0017</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>781.052081</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222343</td>\n",
       "      <td>1.366347</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.1840</td>\n",
       "      <td>90.0186</td>\n",
       "      <td>6.1838</td>\n",
       "      <td>89.9980</td>\n",
       "      <td>23.6287</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0025</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>782.500110</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.038339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.167293</td>\n",
       "      <td>1.320101</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.7510</td>\n",
       "      <td>90.9688</td>\n",
       "      <td>5.6595</td>\n",
       "      <td>91.1228</td>\n",
       "      <td>13.9630</td>\n",
       "      <td>train</td>\n",
       "      <td>30.5185</td>\n",
       "      <td>3</td>\n",
       "      <td>227</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>391.227531</td>\n",
       "      <td>0.102242</td>\n",
       "      <td>0.083072</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196553</td>\n",
       "      <td>1.469992</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0036</td>\n",
       "      <td>89.9888</td>\n",
       "      <td>5.0034</td>\n",
       "      <td>90.0119</td>\n",
       "      <td>13.5318</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0017</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>293.377334</td>\n",
       "      <td>0.102257</td>\n",
       "      <td>0.076693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049266</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.6614</td>\n",
       "      <td>89.9960</td>\n",
       "      <td>6.6612</td>\n",
       "      <td>90.0006</td>\n",
       "      <td>24.5813</td>\n",
       "      <td>train</td>\n",
       "      <td>119.9893</td>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375</td>\n",
       "      <td>944.713843</td>\n",
       "      <td>0.084682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.031756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          E        Eg  Natoms       a    alpha       b     beta        c  \\\n",
       "0  0.065788  1.490362    80.0  9.9523  90.0026  8.5513  90.0023   9.1775   \n",
       "1  0.222343  1.366347    80.0  6.1840  90.0186  6.1838  89.9980  23.6287   \n",
       "2  0.167293  1.320101    40.0  9.7510  90.9688  5.6595  91.1228  13.9630   \n",
       "3  0.196553  1.469992    30.0  5.0036  89.9888  5.0034  90.0119  13.5318   \n",
       "4  0.049266  0.866806    80.0  6.6614  89.9960  6.6612  90.0006  24.5813   \n",
       "\n",
       "  dataset     gamma  id   sg    x_Al    x_Ga   x_In         vol   density  \\\n",
       "0   train   90.0017   1   33  0.6250  0.3750  0.000  781.052081  0.102426   \n",
       "1   train  120.0025   2  194  0.6250  0.3750  0.000  782.500110  0.102236   \n",
       "2   train   30.5185   3  227  0.8125  0.1875  0.000  391.227531  0.102242   \n",
       "3   train  120.0017   4  167  0.7500  0.0000  0.250  293.377334  0.102257   \n",
       "4   train  119.9893   5  194  0.0000  0.6250  0.375  944.713843  0.084682   \n",
       "\n",
       "   density_Al  density_Ga  density_In  \n",
       "0    0.064016    0.038410    0.000000  \n",
       "1    0.063898    0.038339    0.000000  \n",
       "2    0.083072    0.019170    0.000000  \n",
       "3    0.076693    0.000000    0.025564  \n",
       "4    0.000000    0.052926    0.031756  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pymatgen\n",
    "elem_prop=pd.concat([pd.Series(pymatgen.Element.Al.data,name='Al'), pd.Series(pymatgen.Element.Ga.data,name='Ga'),pd.Series(pymatgen.Element.In.data,name='In')], axis=1)\n",
    "keep=['Atomic mass','Atomic radius','Atomic radius calculated','Density of solid','Ionic radii','Molar volume','Thermal conductivity','Van der waals radius','Velocity of sound','X' ]\n",
    "keep_names=['a_mass','a_radius','a_radius_c','elem_density','ionic_r','molar_v','t_cond','vdw_radius','sound_v','X' ]\n",
    "#keep=['Atomic mass']\n",
    "elem_prop=elem_prop.T\n",
    "elem_prop2=elem_prop[keep]\n",
    "elem_prop2['Ionic radii']=elem_prop2['Ionic radii'].apply(lambda x: (list(x.values()))[0])\n",
    "for c in ['Molar volume','Density of solid','Thermal conductivity','Velocity of sound']:\n",
    "    elem_prop2[c]=elem_prop2[c].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "for c in   elem_prop2.columns:\n",
    "     elem_prop2[c]=elem_prop2[c].astype(\"float32\")\n",
    "\n",
    "elem_prop2.rename(columns={old:new for (old,new) in zip(keep,keep_names)},inplace=True)\n",
    "\n",
    "print(elem_prop2)\n",
    "\n",
    "def avg_prop(x_Al, x_Ga, x_In, prop):\n",
    "    return elem_prop2.loc['Al',prop]*x_Al+elem_prop2.loc['Ga',prop]*x_Ga+elem_prop2.loc['In',prop]*x_In\n",
    "\n",
    "properties=elem_prop2.columns\n",
    "\n",
    "for prop in properties:\n",
    "    df_total['_'.join(['avg', prop])] = avg_prop(df_train['x_Al'], df_train['x_Ga'],df_train['x_In'],prop)\n",
    "    \n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encoding of cat features\n",
    "import sys \n",
    "sys.path.append(\"../kaggle_varie\")\n",
    "from  varie import *\n",
    "cols_to_enc=[\"sg\"]\n",
    "\n",
    "#binary encoder\n",
    "#enc=bin_enc(df_total,cols_to_enc,verbose=2,copy=True,drop_original=True,ordinal_only=False)\n",
    "#one-hot encoder\n",
    "enc=pd.get_dummies(df_total,columns=cols_to_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_fct(model,params,df,y_col,n_iter=20,cv=4,drop_col=[],verbose=2):\n",
    "    \n",
    "    X_train=df.drop(y_col+drop_col,axis=1).values\n",
    "    grids=[]\n",
    "    for y in y_col:\n",
    "        print(y)\n",
    "        y_train=df[y].values\n",
    "        print(X_train.shape,y_train.shape)\n",
    "\n",
    "        grid=RandomizedSearchCV(model,param_distributions=params, n_iter=n_iter,cv=cv,verbose=verbose,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "        grid.fit(X_train,y_train)\n",
    "        grids.append(grid)\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%aimport` not found.\n"
     ]
    }
   ],
   "source": [
    "#grid search for random forest\n",
    "import scipy\n",
    "from  sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import *\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import  ElasticNet\n",
    "from sklearn.ensemble import  GradientBoostingRegressor, RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from sklearn.kernel_approximation import Nystroem\n",
    "#from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import Lasso,Ridge,LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from varie import lognuniform\n",
    "#from varie import loguniform2\n",
    "%aimport varie\n",
    "import varie\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "y_col=[\"E\",\"Eg\"]\n",
    "drop_col=[\"id\",\"dataset\"]\n",
    "df_total_train_eval=enc[df_total.dataset=='train']\n",
    "df_total_test=enc[df_total.dataset=='test']\n",
    "\n",
    "X_train=df_total_train_eval.drop(y_col+drop_col,axis=1).values\n",
    "X_test=df_total_test.drop(y_col+drop_col,axis=1).values\n",
    "\n",
    "models={\n",
    "    \n",
    "    'knn':\n",
    "           (KNeighborsRegressor(),\n",
    "            {'n_neighbors':scipy.stats.randint(1,100)}),\n",
    "    \n",
    "    'svr':\n",
    "           (SVR(verbose=False,kernel='linear'),\n",
    "            {'C':lognuniform(low=-4,high=4,base=10,size=100),\n",
    "             'epsilon':lognuniform(low=-2,high=0,base=10,size=100)}),\n",
    "    \n",
    "    'svr_rbf':\n",
    "           (SVR(verbose=False,kernel='rbf'),\n",
    "            {'C': lognuniform(low=-2,high=2,base=10,size=100),\n",
    "             'gamma':lognuniform(low=-2,high=2,base=10,size=100)}),\n",
    "\n",
    "    'rf':\n",
    "           (ensemble.RandomForestRegressor(verbose=False),\n",
    "            {\"max_depth\": scipy.stats.randint(1,100), \n",
    "             'n_estimators': scipy.stats.randint(1,400),\n",
    "             'max_features':('log2','sqrt','auto'),\n",
    "             'min_samples_split':scipy.stats.randint(2,5),\n",
    "             'min_samples_leaf':scipy.stats.randint(1,5)}),\n",
    "    \n",
    "    'cb':\n",
    "           (CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE',logging_level='Silent'),\n",
    "            {\"depth\": scipy.stats.randint(1,6), \n",
    "             'iterations': scipy.stats.randint(100,2000),\n",
    "             'learning_rate':lognuniform(low=-2,high=-1,base=10,size=100),\n",
    "             'l2_leaf_reg': scipy.stats.randint(2,4)}),\n",
    "    \n",
    "    'mlp': \n",
    "           (MLPRegressor((80, 10), early_stopping=False),\n",
    "             {'hidden_layer_sizes':scipy.stats.randint(1,100),\n",
    "              'alpha':lognuniform(low=-5,high=-1,base=10,size=100)}),\n",
    "             \n",
    "     'gb':\n",
    "           (GradientBoostingRegressor(n_estimators=100),\n",
    "            {'learning_rate':lognuniform(low=-3,high=-1,base=10,size=100), \n",
    "             'n_estimators': scipy.stats.randint(1,300),\n",
    "             'max_depth':scipy.stats.randint(1,5),\n",
    "             'max_features':('sqrt','log2','auto')}),\n",
    "    \n",
    "    'lasso':\n",
    "            (Lasso(),\n",
    "            {'alpha':lognuniform(low=-6,high=2,base=10,size=100)}),  \n",
    "\n",
    "    'ridge':\n",
    "            (Ridge(),\n",
    "            {'alpha':varie.lognuniform(low=-6,high=2,base=10,size=100)}),\n",
    "    \n",
    "    'eln':\n",
    "            (ElasticNet(),\n",
    "            {'alpha':lognuniform(low=-6,high=4,base=10,size=100), \n",
    "             'l1_ratio':lognuniform(low=-6,high=4,base=10,size=100)}),\n",
    "    \n",
    "    'xgb':\n",
    "        (XGBRegressor(),\n",
    "         {'max_depth':scipy.stats.randint(1,100), \n",
    "          'learning_rate':lognuniform(low=-4,high=-0.5,base=10,size=100), \n",
    "          'n_estimators':scipy.stats.randint(1,400),\n",
    "          'colsample_bytree': uniform(0.55, 0.66),\n",
    "          'min_child_weight': randint(30, 60),\n",
    "          'colsample_bytree': uniform(0.6, 0.4),\n",
    "          'reg_lambda': uniform(1, 2),\n",
    "          'reg_alpha': uniform(1, 2),\n",
    "}),\n",
    "    \n",
    " #does not install    \n",
    "    'gbm' :\n",
    "        (LGBMRegressor(objective='regression'),\n",
    "            {'num_leaves':scipy.stats.randint(1,200), \n",
    "          'learning_rate':lognuniform(low=-4,high=-0.5,base=10,size=100), \n",
    "          'n_estimators':scipy.stats.randint(1,400)}),\n",
    "    'adb' :\n",
    "        (AdaBoostRegressor(loss=\"square\"),\n",
    "            {'learning_rate':lognuniform(low=-4,high=-0.1,base=10,size=10), \n",
    "             'n_estimators':scipy.stats.randint(1,400)}),         \n",
    "\n",
    "    \n",
    "       }\n",
    " \n",
    "    \n",
    "\n",
    "try:\n",
    "    results\n",
    "except:\n",
    "    results={}\n",
    "    \n",
    "for (tag,model) in  models.items():\n",
    "    if (tag not in results):\n",
    "        print(tag)\n",
    "        results[tag]=grid_search_fct(model[0],model[1],df_total_train_eval,y_col,n_iter=10,cv=4,drop_col=drop_col,verbose=1)\n",
    "\n",
    "\n",
    "    \n",
    "    #grid=RandomizedSearchCV(model[0],param_distributions=params, n_iter=20,cv=4,verbose=2,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "                        \n",
    "    #grid.fit(X_train,y_train)\n",
    "    #grids.append(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(results, open( \"results_100iter.pickle\", \"wb\" ))\n",
    "#results.pop('cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "{'n_neighbors': 10}\n",
      "{'n_neighbors': 8}\n",
      "0.0844665177528 0.0583265454341 0.110606490071\n",
      "svr\n",
      "{'epsilon': 0.019614171120463306, 'C': 0.35193348838838973}\n",
      "{'epsilon': 0.019614171120463306, 'C': 3.5803832611521806}\n",
      "0.0967069025043 0.0556463496366 0.137767455372\n",
      "rf\n",
      "{'max_depth': 44, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 378}\n",
      "{'max_depth': 92, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 397}\n",
      "0.062052700501 0.0330543963934 0.0910510046087\n",
      "cb\n",
      "{'depth': 5, 'iterations': 1266, 'l2_leaf_reg': 2, 'learning_rate': 0.017966319176400233}\n",
      "{'depth': 5, 'iterations': 1283, 'l2_leaf_reg': 3, 'learning_rate': 0.022438886398284608}\n",
      "0.0593270482992 0.0318770563238 0.0867770402745\n",
      "mlp\n",
      "{'alpha': 7.4480758285059003e-05, 'hidden_layer_sizes': 74}\n",
      "{'alpha': 0.00019138373871388064, 'hidden_layer_sizes': 36}\n",
      "0.181769505703 0.141626186076 0.221912825331\n",
      "gb\n",
      "{'learning_rate': 0.083379899825578627, 'max_depth': 3, 'max_features': 'sqrt', 'n_estimators': 276}\n",
      "{'learning_rate': 0.078515382984050711, 'max_depth': 3, 'max_features': 'sqrt', 'n_estimators': 294}\n",
      "0.0607323889728 0.0330847136665 0.0883800642791\n",
      "lasso\n",
      "{'alpha': 1.1879576324595536e-06}\n",
      "{'alpha': 8.0375259844445635e-06}\n",
      "0.0668519515706 0.0429623242473 0.0907415788939\n",
      "ridge\n",
      "{'alpha': 0.00033928659333169823}\n",
      "{'alpha': 0.00091939115375154121}\n",
      "0.0667033369419 0.042984347287 0.0904223265969\n",
      "eln\n",
      "{'l1_ratio': 4.28567676527258, 'alpha': 0.00037822592338935016}\n",
      "{'l1_ratio': 0.0028638571321583026, 'alpha': 1.9272759987467041e-06}\n",
      "0.0755722994288 0.0605250809401 0.0906195179175\n",
      "xgb\n",
      "{'colsample_bytree': 0.76307605230482012, 'learning_rate': 0.12569747681153476, 'max_depth': 55, 'min_child_weight': 57, 'n_estimators': 64, 'reg_alpha': 1.328420642426531, 'reg_lambda': 1.1209687741930541}\n",
      "{'colsample_bytree': 0.62301211740258189, 'learning_rate': 0.065447264081011855, 'max_depth': 14, 'min_child_weight': 50, 'n_estimators': 378, 'reg_alpha': 1.8838567598458908, 'reg_lambda': 2.7159002275233588}\n",
      "0.061640140226 0.0343795096437 0.0889007708083\n",
      "adb\n",
      "{'learning_rate': 0.0041964894561541096, 'n_estimators': 395}\n",
      "{'learning_rate': 0.018430097003204731, 'n_estimators': 228}\n",
      "0.0798760506809 0.04693097278 0.112821128582\n",
      "svr_rbf\n",
      "{'gamma': 0.19118563114170414, 'C': 31.187297271363082}\n",
      "{'gamma': 0.3445814274991652, 'C': 0.77957863060320076}\n",
      "0.124645548817 0.0730727802467 0.176218317387\n",
      "gbm\n",
      "{'learning_rate': 0.022250087444538037, 'n_estimators': 326, 'num_leaves': 13}\n",
      "{'learning_rate': 0.15152891488571471, 'n_estimators': 392, 'num_leaves': 11}\n",
      "0.0644786237069 0.0322991542291 0.0966580931847\n"
     ]
    }
   ],
   "source": [
    "#best models and their performance\n",
    "\n",
    "for tag,grids in results.items():\n",
    "    print(tag)\n",
    "    for grid in grids:\n",
    "        print(grid.best_params_)\n",
    "    print((np.sqrt(-grids[0].best_score_)+np.sqrt(-grids[1].best_score_))/2,\n",
    "          np.sqrt(-grids[0].best_score_),np.sqrt(-grids[1].best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:122: DeprecationWarning: Estimator CatBoostRegressor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15, score=-0.001332, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15, score=-0.001181, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15, score=-0.001291, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15, score=-0.001137, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  6.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=67, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=15, score=-0.001343, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233, score=-0.001341, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 10.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233, score=-0.001168, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 12.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233, score=-0.001325, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 14.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233, score=-0.001165, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 16.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=auto, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=233, score=-0.001355, total= 2.2min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225, score=-0.001330, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225, score=-0.001141, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225, score=-0.001320, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225, score=-0.001152, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=80, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=225, score=-0.001319, total= 2.0min\n",
      "[CV] meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93 \n",
      "[CV]  meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93, score=-0.001291, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93 \n",
      "[CV]  meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93, score=-0.001149, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93 \n",
      "[CV]  meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93, score=-0.001289, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93 \n",
      "[CV]  meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93, score=-0.001145, total= 1.6min\n",
      "[CV] meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93 \n",
      "[CV]  meta-randomforestregressor__max_depth=72, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=93, score=-0.001312, total= 1.6min\n",
      "[CV] meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63 \n",
      "[CV]  meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63, score=-0.001256, total= 1.5min\n",
      "[CV] meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63 \n",
      "[CV]  meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63, score=-0.001074, total= 1.5min\n",
      "[CV] meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63 \n",
      "[CV]  meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63, score=-0.001260, total= 1.6min\n",
      "[CV] meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63 \n",
      "[CV]  meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63, score=-0.001122, total= 1.4min\n",
      "[CV] meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63 \n",
      "[CV]  meta-randomforestregressor__max_depth=92, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=63, score=-0.001307, total= 2.1min\n",
      "[CV] meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45 \n",
      "[CV]  meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45, score=-0.001262, total= 2.5min\n",
      "[CV] meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45, score=-0.001069, total= 2.6min\n",
      "[CV] meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45 \n",
      "[CV]  meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45, score=-0.001268, total= 2.4min\n",
      "[CV] meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45 \n",
      "[CV]  meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45, score=-0.001099, total= 2.2min\n",
      "[CV] meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45 \n",
      "[CV]  meta-randomforestregressor__max_depth=73, meta-randomforestregressor__max_features=log2, meta-randomforestregressor__min_samples_leaf=4, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=45, score=-0.001289, total= 2.0min\n",
      "[CV] meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91 \n",
      "[CV]  meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91, score=-0.001291, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91 \n",
      "[CV]  meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91, score=-0.001126, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91 \n",
      "[CV]  meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91, score=-0.001274, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91 \n",
      "[CV]  meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91, score=-0.001144, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91 \n",
      "[CV]  meta-randomforestregressor__max_depth=18, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=2, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=91, score=-0.001321, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225, score=-0.001276, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225, score=-0.001096, total= 1.9min\n",
      "[CV] meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225, score=-0.001247, total= 1.8min\n",
      "[CV] meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225, score=-0.001117, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225 \n",
      "[CV]  meta-randomforestregressor__max_depth=35, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=3, meta-randomforestregressor__min_samples_split=3, meta-randomforestregressor__n_estimators=225, score=-0.001303, total= 1.4min\n",
      "[CV] meta-randomforestregressor__max_depth=71, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=249 \n",
      "[CV]  meta-randomforestregressor__max_depth=71, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=249, score=-0.001316, total= 1.5min\n",
      "[CV] meta-randomforestregressor__max_depth=71, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=249 \n",
      "[CV]  meta-randomforestregressor__max_depth=71, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=249, score=-0.001154, total= 1.7min\n",
      "[CV] meta-randomforestregressor__max_depth=71, meta-randomforestregressor__max_features=sqrt, meta-randomforestregressor__min_samples_leaf=1, meta-randomforestregressor__min_samples_split=4, meta-randomforestregressor__n_estimators=249 \n"
     ]
    }
   ],
   "source": [
    "#Stacking via mlxtend\n",
    "from mlxtend import StackingRegressor\n",
    "#cbr=CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE',logging_level='Silent')\n",
    "#    'rf':\n",
    "rf1=ensemble.RandomForestRegressor(verbose=False)\n",
    "rf2=ensemble.RandomForestRegressor(verbose=False)\n",
    "#abc = SVR(kernel='rbf')\n",
    "\n",
    "params_meta={\"meta-randomforestregressor__max_depth\": scipy.stats.randint(1,100), \n",
    "             'meta-randomforestregressor__n_estimators': scipy.stats.randint(1,400),\n",
    "             'meta-randomforestregressor__max_features':('log2','sqrt','auto'),\n",
    "             'meta-randomforestregressor__min_samples_split':scipy.stats.randint(2,5),\n",
    "             'meta-randomforestregressor__min_samples_leaf':scipy.stats.randint(1,5)}\n",
    "\n",
    "\n",
    "#grid search\n",
    "#params_meta = {'meta-svr__C': [0.1, 1.0, 10.0, 100.0],\n",
    "#          'meta-svr__gamma': [0.1, 1.0, 10.0]}\n",
    "\n",
    "#randomized search\n",
    "#params_meta = {'meta-svr__C': lognuniform(low=-2,high=2,base=10,size=100),\n",
    "#               'meta-svr__gamma': lognuniform(low=-2,high=2,base=10,size=100)}\n",
    "\n",
    "#{'meta-cbr__depth': scipy.stats.randint(1,6)}\n",
    "             \n",
    "             \n",
    "             #'meta-meta_learner__iterations': scipy.stats.randint(100,2000),\n",
    "             #'meta-meta_learner__learning_rate':lognuniform(low=-2,high=-1,base=10,size=100),\n",
    "             #'meta-meta_learner__l2_leaf_reg': scipy.stats.randint(2,4)}\n",
    "            \n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "learners1=[g[0].best_estimator_ for g in results.values()]\n",
    "learners2=[g[1].best_estimator_ for g in results.values()]\n",
    "learners=[learners1,learners2]\n",
    "\n",
    "stregr = [StackingRegressor(regressors=learners1,meta_regressor=rf1),\n",
    "          StackingRegressor(regressors=learners2,meta_regressor=rf2)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results2=[]\n",
    "for s,y in zip(stregr,y_col):\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y]\n",
    "    grid=RandomizedSearchCV(s,param_distributions=params_meta, n_iter=10,cv=5,verbose=10,scoring=\"neg_mean_squared_error\" )\n",
    "    #grid=GridSearchCV(s,param_grid=params_meta, cv=5,verbose=10,scoring=\"neg_mean_squared_error\" )\n",
    "    grid.fit(X_train, y_train)\n",
    "    results2.append(grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingRegressor.get_params(StackingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "import mlens\n",
    "from mlens.model_selection import Evaluator\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.metrics import rmse\n",
    "\n",
    "from mlens.metrics import make_scorer\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "\n",
    "learners1=[g[0].best_estimator_ for g in results.values()]\n",
    "learners2=[g[1].best_estimator_ for g in results.values()]\n",
    "learners=[learners1,learners2]\n",
    "\n",
    "# Instantiate the ensemble with 10 folds\n",
    "#meta_learner1=CatBoostRegressor(iterations=1200,\n",
    "#                            learning_rate=0.03,\n",
    "#                            depth=4,\n",
    "#                            loss_function='RMSE',\n",
    "#                            eval_metric='RMSE',\n",
    "##                            random_seed=SEED,\n",
    "#                            od_type='Iter',\n",
    "#                            od_wait=50,verbose=False)\n",
    "\n",
    "#import copy\n",
    "#meta_learner2=copy.deepcopy(meta_learner1)\n",
    "\n",
    "#sl1 = SuperLearner(\n",
    "#    folds=5,\n",
    "#    verbose=True,\n",
    "##    scorer=mlens.metrics.rmse\n",
    "#)\n",
    "#sl2 = SuperLearner(\n",
    "#    folds=5,\n",
    "#    verbose=True,\n",
    "#    scorer=mlens.metrics.rmse\n",
    "#)\n",
    "\n",
    "# Add the base learners and the meta learner\n",
    "#sl1.add(learners1) \n",
    "#sl1.add_meta(meta_learner1)\n",
    "#sl2.add(learners2) \n",
    "#sl2.add_meta(meta_learner2)\n",
    "\n",
    "#sls=[sl1,sl2]\n",
    "#evaluator\n",
    "#evl = Evaluator(make_scorer(mlens.metrics.rmse), cv=5, shuffle=False)\n",
    "sls=[]\n",
    "for learner,y in zip(learners,y_col):\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    \n",
    "    #evl.fit(X_train, y_train, sl, {}, n_iter=1)\n",
    "    from mlens.ensemble import SuperLearner\n",
    "    \n",
    "    sl = SuperLearner(\n",
    "    folds=5,\n",
    "    verbose=True,\n",
    "    #    scorer=mlens.metrics.rmse\n",
    "    )\n",
    "    \n",
    "    meta_learner=CatBoostRegressor(iterations=1200,\n",
    "                            learning_rate=0.03,\n",
    "                            depth=4,\n",
    "                            loss_function='RMSE',\n",
    "                            eval_metric='RMSE',\n",
    "#                            random_seed=SEED,\n",
    "                            od_type='Iter',\n",
    "                            od_wait=50,verbose=False)\n",
    "    \n",
    "    sl.add(learner) \n",
    "    sl.add_meta(meta_learner)\n",
    "    # Train the ensemble\n",
    "    sl.fit(X_train, y_train)\n",
    "    preds = sl.predict(X_train)\n",
    "    print(rmse(y_train, preds))\n",
    "    sls.append(sl)\n",
    "#    results.append(mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "#                          evl.summary['test_score_mean']['superlearner'],\n",
    "#                          evl.summary['test_score_std']['superlearner'],\n",
    "#                          mlens.metrics.rmse(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "#    print_scores(scores_df, 'mlens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from mlens.ensemble import SuperLearner\n",
    "import mlens\n",
    "from mlens.model_selection import Evaluator\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.metrics import rmse\n",
    "\n",
    "\n",
    "\n",
    "learners1=[grid[0].best_estimator_ for grid in results.values()]\n",
    "learners2=[grid[1].best_estimator_ for grid in results.values()]\n",
    "\n",
    "grid_sl=[]\n",
    "\n",
    "tries=3\n",
    "#grid search for the meta learner         \n",
    "for depth, iterations, learning_rate in zip (scipy.stats.randint(1,5).rvs(tries),\n",
    "                                              scipy.stats.randint(1000,2000).rvs(tries),    \n",
    "                                              lognuniform(low=-2,high=-1,base=10,size=tries)):\n",
    "    print(depth, iterations, learning_rate)\n",
    "    \n",
    "\n",
    "    # Instantiate the ensemble with 10 folds\n",
    "    meta_learner1=CatBoostRegressor(iterations=iterations,\n",
    "                                learning_rate=learning_rate,\n",
    "                                depth=depth,\n",
    "                                loss_function='RMSE',\n",
    "                                eval_metric='RMSE',\n",
    "    #                            random_seed=SEED,\n",
    "                                od_type='Iter',\n",
    "                                od_wait=50,verbose=False)\n",
    "\n",
    "    import copy\n",
    "    meta_learner2=copy.deepcopy(meta_learner1)\n",
    "\n",
    "    sl1 = SuperLearner(\n",
    "        folds=5,\n",
    "        verbose=True,\n",
    "        scorer=mlens.metrics.rmse\n",
    "    )\n",
    "    sl2 = SuperLearner(\n",
    "        folds=5,\n",
    "        verbose=True,\n",
    "        scorer=mlens.metrics.rmse\n",
    "    )\n",
    "\n",
    "    # Add the base learners and the meta learner\n",
    "    sl1.add(learners1) \n",
    "    sl1.add_meta(meta_learner1)\n",
    "    sl2.add(learners2) \n",
    "    sl2.add_meta(meta_learner2)\n",
    "\n",
    "\n",
    "\n",
    "    sls=[sl1,sl2]\n",
    "    #evaluator\n",
    "    #evl = Evaluator(make_scorer(mlens.metrics.rmse), cv=5, shuffle=False)\n",
    "\n",
    "    for i,y in enumerate(y_col):\n",
    "        print(y)\n",
    "        y_train=df_total_train_eval[y].values\n",
    "        #print(X_train.shape,y_train.shape)\n",
    "\n",
    "        #evl.fit(X_train, y_train, sl, {}, n_iter=1)\n",
    "\n",
    "        # Train the ensemble\n",
    "        sls[i].fit(X_train, y_train)\n",
    "        preds = sls[i].predict(X_train)\n",
    "        print(rmse(y_train, preds))\n",
    "        \n",
    "        grid_sl.append(depth, iterations, learning_rate,sls)\n",
    "        \n",
    "        \n",
    "    #    results.append(mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "    #                          evl.summary['test_score_mean']['superlearner'],\n",
    "    #                          evl.summary['test_score_std']['superlearner'],\n",
    "    #                          mlens.metrics.rmse(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "    #    print_scores(scores_df, 'mlens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_layer = SuperLearner(model_selection=True)\n",
    "in_layer.add(base_learners)\n",
    "\n",
    "preprocess = [in_layer]\n",
    "\n",
    "evl = Evaluator(\n",
    "    scorer,\n",
    "    cv=2,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "evl.fit(\n",
    "    X_train, y_train,\n",
    "    meta_learners,\n",
    "    param_dicts,\n",
    "    preprocessing={'meta': preprocess},\n",
    "    n_iter=5                           # bump this up to do a larger grid search\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write to csv\n",
    "%load_ext autoreload\n",
    "%aimport varie\n",
    "%autoreload 2\n",
    "#I use a different model for E and Eg\n",
    "varie.make_csv2(df_total_train_eval,pd.DataFrame(),df_total_test,\n",
    "#         (ensemble.RandomForestRegressor(max_depth= 11, max_features='log2', n_estimators= 55),\n",
    "#          ensemble.RandomForestRegressor(max_depth= 9, max_features='sqrt', n_estimators= 220)),\n",
    "            (sl1,sl2),\n",
    "         y_col,'sl3.csv',drop=drop_col,columns=['id','E','Eg'],\n",
    "         new_column_names=['id','formation_energy_ev_natom' ,'bandgap_energy_ev'],change_col_names=True,fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grids_sl=[]\n",
    "for i,y in enumerate(y_col):\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "\n",
    "    grid=RandomizedSearchCV(model,param_distributions=params, n_iter=n_iter,cv=cv,verbose=verbose,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "    grid.fit(X_train,y_train)\n",
    "    grids_sl.append(grid)\n",
    "\n",
    "    sls[i].fit(X_train, y_train)\n",
    "    preds = sls[i].predict(X_train)\n",
    "    print(rmse(y_train, preds))\n",
    "#    results.append(mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "#                          evl.summary['test_score_mean']['superlearner'],\n",
    "#                          evl.summary['test_score_std']['superlearner'],\n",
    "#                          mlens.metrics.rmse(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "#    print_scores(scores_df, 'mlens')   X_train=df.drop(y_col+drop_col,axis=1).values\n",
    "    for y in y_col:\n",
    "\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importance of each feature\n",
    "non_features = y_col+drop_col\n",
    "features = [col for col in list(df_total_train_eval) if col not in non_features]\n",
    "importances_E =  grids[0].best_estimator_.feature_importances_\n",
    "\n",
    "assert(len(importances_E)==len(features))\n",
    "\n",
    "descending_indices_E = np.argsort(importances_E)[::-1]\n",
    "sorted_importances_E = [importances_E[idx] for idx in descending_indices_E]\n",
    "sorted_features_E = [features[idx] for idx in descending_indices_E]\n",
    "print('most important feature for formation energy is %s' % sorted_features_E[0])\n",
    "for f, i in zip(sorted_features_E,sorted_importances_E):\n",
    "    print('E',f,i)\n",
    "    \n",
    "importances_Eg =  grids[1].best_estimator_.feature_importances_\n",
    "descending_indices_Eg = np.argsort(importances_Eg)[::-1]\n",
    "sorted_importances_Eg = [importances_Eg[idx] for idx in descending_indices_Eg]\n",
    "sorted_features_Eg = [features[idx] for idx in descending_indices_Eg]\n",
    "print('most important feature for band gap is %s' % sorted_features_Eg[0])\n",
    "\n",
    "\n",
    "    \n",
    "for f, i in zip(sorted_features_Eg,sorted_importances_Eg):\n",
    "    print('Eg',f,i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#write to csv\n",
    "%load_ext autoreload\n",
    "%aimport varie\n",
    "%autoreload 2\n",
    "#I use a different model for E and Eg\n",
    "varie.make_csv2(df_total_train_eval,pd.DataFrame(),df_total_test,\n",
    "#         (ensemble.RandomForestRegressor(max_depth= 11, max_features='log2', n_estimators= 55),\n",
    "#          ensemble.RandomForestRegressor(max_depth= 9, max_features='sqrt', n_estimators= 220)),\n",
    "            (grids[0].best_estimator_,grids[1].best_estimator_),\n",
    "         y_col,'rf2c.csv',drop=drop_col,columns=['id','E','Eg'],\n",
    "         new_column_names=['id','formation_energy_ev_natom' ,'bandgap_energy_ev'],change_col_names=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from  catboost import CatBoostRegressor\n",
    "def runCatBoost(x_train, y_train,x_test, y_test,test,depth):\n",
    "    model=CatBoostRegressor(iterations=1200,\n",
    "                            learning_rate=0.03,\n",
    "                            depth=depth,\n",
    "                            loss_function='RMSE',\n",
    "                            eval_metric='RMSE',\n",
    "                            random_seed=99,\n",
    "                            od_type='Iter',\n",
    "                            od_wait=50)\n",
    "    model.fit(x_train, y_train, eval_set=(x_test, y_test), use_best_model=True, verbose=False)\n",
    "    y_pred_train=model.predict(x_test)\n",
    "    rmsle_result = rmsle(np.exp(y_pred_train)-1,np.exp(y_test)-1)\n",
    "    y_pred_test=model.predict(test)\n",
    "    return y_pred_train,rmsle_result,y_pred_test\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A host of Scikit-learn models\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from catboost import CatBoostRegressor\n",
    "SEED=1\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    #nb = GaussianNB()\n",
    "    svc = SVR(C=100)\n",
    "    knn = KNeighborsRegressor(n_neighbors=3)\n",
    "    nn = MLPRegressor((80, 10), early_stopping=False, random_state=SEED)\n",
    "    gb = GradientBoostingRegressor(n_estimators=100, random_state=SEED)\n",
    "    rf = RandomForestRegressor(n_estimators=250, max_features='sqrt', max_depth=100, random_state=SEED)\n",
    "    cb= CatBoostRegressor(depth=4,iterations=1200,learning_rate=0.03,verbose=False)\n",
    "\n",
    "    models = {'svm': svc,\n",
    "              'knn': knn,\n",
    "              'mlp-nn': nn,\n",
    "              'random forest': rf,\n",
    "              'gbm': gb,\n",
    "              'catboost':cb\n",
    "              }\n",
    "\n",
    "    return models\n",
    "\n",
    "base_learners = get_models()\n",
    "\n",
    "\n",
    "#meta_learner = GradientBoostingRegressor(\n",
    "#    n_estimators=1000,\n",
    "#    loss=\"exponential\",\n",
    "#    max_features=4,\n",
    "#    max_depth=3,\n",
    "#    subsample=0.5,\n",
    "#    learning_rate=0.005, \n",
    "#    random_state=SEED\n",
    "#)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ensemble = SuperLearner(folds=5, scorer=mse)\n",
    "#ensemble.add([xgb.XGBRegressor(), lgb.LGBMRegressor(n_estimators=30)])\n",
    "#ensemble.add_meta([LinearRegression()])\n",
    "\n",
    "evl = Evaluator(make_scorer(mlens.metrics.rmse), cv=5, shuffle=False)\n",
    "evl.fit(X_train, y_train, sl, {}, n_iter=1)\n",
    "\n",
    "\n",
    "scores_df['mlens'] = [mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "                      evl.summary['test_score_mean']['superlearner'],\n",
    "                      evl.summary['test_score_std']['superlearner'],\n",
    "                      mlens.metrics.rmse(y_test, ensemble.predict(X_test))]\n",
    "\n",
    "print_scores(scores_df, 'mlens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "evaluator.fit(X_train, y_train,sl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for y in y_col:\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    y_test=df_total_test[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    \n",
    "\n",
    "    model=CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE',verbose=False)\n",
    "    params= {\"depth\": scipy.stats.randint(1,5), \n",
    "             'iterations': scipy.stats.randint(1000,2000),\n",
    "             'learning_rate':lognuniform(low=-2,high=-1,base=10,size=100)}\n",
    "    \n",
    "    \n",
    "    grid=RandomizedSearchCV(sl,param_distributions=params, n_iter=20,cv=4,verbose=2,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "                        \n",
    "    grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ests=[(tag, model[0]) for tag,model in models.items()]\n",
    "params={tag:model[1] for tag,model in models.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlens.model_selection import Evaluator\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Here we name the estimators ourselves\n",
    "#ests = [('gnb', GaussianNB()), ('knn', KNeighborsClassifier())]\n",
    "\n",
    "# Now we map parameters to these\n",
    "# The gnb doesn't have any parameters so we can skip it\n",
    "#pars = {'n_neighbors': randint(2, 20)}\n",
    "#params = {'knn': pars}\n",
    "\n",
    "evaluators=[]\n",
    "for i,y in enumerate(y_col):\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    evaluator = Evaluator(rmse_scorer, cv=10,  verbose=1)\n",
    "\n",
    "    \n",
    "    evaluator.fit(X_train,y_train, ests, params, n_iter=5)\n",
    "    evaluators.append(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlens.metrics import make_scorer\n",
    "rmse_scorer = make_scorer(rmse, average='micro', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_learners=[(tag,model[0]) for tag,model in models.items()]\n",
    "param_dicts_base={tag:model[1] for tag,model in models.items()}\n",
    "len(param_dicts_base),len(base_learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "SEED=1\n",
    "# We consider the following models (or base learners)\n",
    "gb = XGBRegressor()\n",
    "ls = Lasso(alpha=1e-6, normalize=True)\n",
    "el = ElasticNet(alpha=1e-6, normalize=True)\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "base_learners = [\n",
    "    ('ls', ls), ('el', el), ('rf', rf), ('gb', gb)\n",
    "]\n",
    "\n",
    "# Put their parameter dictionaries in a dictionary with the\n",
    "# estimator names as keys\n",
    "param_dicts_base = {\n",
    "    'ls':\n",
    "    {'alpha': uniform(1e-6, 1e-5)},\n",
    "    'el':\n",
    "    {'alpha': uniform(1e-6, 1e-5),\n",
    "     'l1_ratio': uniform(0, 1)\n",
    "    },\n",
    "    'gb':\n",
    "    {'learning_rate': uniform(0.02, 0.04),\n",
    "     'colsample_bytree': uniform(0.55, 0.66),\n",
    "     'min_child_weight': randint(30, 60),\n",
    "     'max_depth': randint(3, 7),\n",
    "     'subsample': uniform(0.4, 0.2),\n",
    "     'n_estimators': randint(150, 200),\n",
    "     'colsample_bytree': uniform(0.6, 0.4),\n",
    "     'reg_lambda': uniform(1, 2),\n",
    "     'reg_alpha': uniform(1, 2),\n",
    "    },\n",
    "    'rf':\n",
    "    {'max_depth': randint(2, 5),\n",
    "     'min_samples_split': randint(5, 20),\n",
    "     'min_samples_leaf': randint(10, 20),\n",
    "     'n_estimators': randint(50, 100),\n",
    "     'max_features': uniform(0.6, 0.3)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlens.model_selection import Evaluator\n",
    "assert(len(base_learners)==len(param_dicts_base))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "evl = Evaluator(\n",
    "    scorer,\n",
    "    cv=2,\n",
    "    random_state=SEED,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "evl.fit(\n",
    "    X_train, y_train,\n",
    "    estimators=base_learners,\n",
    "    param_dicts=param_dicts_base,\n",
    "    preprocessing={'sc': [StandardScaler()], 'none': []},\n",
    "    n_iter=2  # bump this up to do a larger grid search\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(evl.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_learners = [\n",
    "    ('gb', gb), ('el', el)\n",
    "]\n",
    "\n",
    "param_dicts = {\n",
    "    'el':\n",
    "    {'alpha': uniform(1e-5, 1),\n",
    "     'l1_ratio': uniform(0, 1)\n",
    "    },\n",
    "    'gb':\n",
    "    {'learning_rate': uniform(0.01, 0.2),\n",
    "     'subsample': uniform(0.5, 0.5),\n",
    "     'reg_lambda': uniform(0.1, 1),\n",
    "     'n_estimators': randint(10, 100)\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Put the layers you don't want to tune into an ensemble with model selection turned on\n",
    "# Just remember to turn it off when you're done!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(meta_learners), len(param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_layer = SuperLearner(model_selection=True)\n",
    "in_layer.add(base_learners)\n",
    "\n",
    "preprocess = [in_layer]\n",
    "\n",
    "evl.fit(\n",
    "    X_train, y_train,\n",
    "    meta_learners,\n",
    "    param_dicts,\n",
    "#    preprocessing={'meta': preprocess},\n",
    "    n_iter=5                           # bump this up to do a larger grid search\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(evl.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lognuniform(low=0, high=1, size=None, base=np.exp(1)):\n",
    "    return np.power(base, np.random.uniform(low, high, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniform.rvs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.uniform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.uniform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sympy.stats import *\n",
    "x = Symbol('x')\n",
    "X = ContinuousRV(x, 2*x, Interval(0, 1))\n",
    "\n",
    "P(X>.5) \n",
    "\n",
    "Var(X) # variance\n",
    "\n",
    "E(2*cos(X)+X**2) # complex expressions are ok too\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
