{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 600, 3000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns={'spacegroup' : 'sg',\n",
    "                            'number_of_total_atoms' : 'Natoms',\n",
    "                            'percent_atom_al' : 'x_Al',\n",
    "                            'percent_atom_ga' : 'x_Ga',\n",
    "                            'percent_atom_in' : 'x_In',\n",
    "                            'lattice_vector_1_ang' : 'a',\n",
    "                            'lattice_vector_2_ang' : 'b',\n",
    "                            'lattice_vector_3_ang' : 'c',\n",
    "                            'lattice_angle_alpha_degree' : 'alpha',\n",
    "                            'lattice_angle_beta_degree' : 'beta',\n",
    "                            'lattice_angle_gamma_degree' : 'gamma',\n",
    "                            'formation_energy_ev_natom' : 'E',\n",
    "                            'bandgap_energy_ev' : 'Eg'}\n",
    "    \n",
    "    \n",
    "df_train = pd.read_csv(\"./input/train.csv\").rename(columns=columns)\n",
    "df_train[\"dataset\"] = \"train\"\n",
    "df_train[\"E\"]=np.log1p(df_train[\"E\"])\n",
    "df_train[\"Eg\"]=np.log1p(df_train[\"Eg\"])\n",
    "df_test = pd.read_csv(\"./input/test.csv\").rename(columns=columns)\n",
    "df_test[\"dataset\"] = \"test\"\n",
    "df_total = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "len(df_train),len(df_test),len(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065788</td>\n",
       "      <td>1.490362</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.9523</td>\n",
       "      <td>90.0026</td>\n",
       "      <td>8.5513</td>\n",
       "      <td>90.0023</td>\n",
       "      <td>9.1775</td>\n",
       "      <td>train</td>\n",
       "      <td>90.0017</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222343</td>\n",
       "      <td>1.366347</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.1840</td>\n",
       "      <td>90.0186</td>\n",
       "      <td>6.1838</td>\n",
       "      <td>89.9980</td>\n",
       "      <td>23.6287</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0025</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.167293</td>\n",
       "      <td>1.320101</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.7510</td>\n",
       "      <td>90.9688</td>\n",
       "      <td>5.6595</td>\n",
       "      <td>91.1228</td>\n",
       "      <td>13.9630</td>\n",
       "      <td>train</td>\n",
       "      <td>30.5185</td>\n",
       "      <td>3</td>\n",
       "      <td>227</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196553</td>\n",
       "      <td>1.469992</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0036</td>\n",
       "      <td>89.9888</td>\n",
       "      <td>5.0034</td>\n",
       "      <td>90.0119</td>\n",
       "      <td>13.5318</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0017</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049266</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.6614</td>\n",
       "      <td>89.9960</td>\n",
       "      <td>6.6612</td>\n",
       "      <td>90.0006</td>\n",
       "      <td>24.5813</td>\n",
       "      <td>train</td>\n",
       "      <td>119.9893</td>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          E        Eg  Natoms       a    alpha       b     beta        c  \\\n",
       "0  0.065788  1.490362    80.0  9.9523  90.0026  8.5513  90.0023   9.1775   \n",
       "1  0.222343  1.366347    80.0  6.1840  90.0186  6.1838  89.9980  23.6287   \n",
       "2  0.167293  1.320101    40.0  9.7510  90.9688  5.6595  91.1228  13.9630   \n",
       "3  0.196553  1.469992    30.0  5.0036  89.9888  5.0034  90.0119  13.5318   \n",
       "4  0.049266  0.866806    80.0  6.6614  89.9960  6.6612  90.0006  24.5813   \n",
       "\n",
       "  dataset     gamma  id   sg    x_Al    x_Ga   x_In  \n",
       "0   train   90.0017   1   33  0.6250  0.3750  0.000  \n",
       "1   train  120.0025   2  194  0.6250  0.3750  0.000  \n",
       "2   train   30.5185   3  227  0.8125  0.1875  0.000  \n",
       "3   train  120.0017   4  167  0.7500  0.0000  0.250  \n",
       "4   train  119.9893   5  194  0.0000  0.6250  0.375  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>24.8145</td>\n",
       "      <td>90.0002</td>\n",
       "      <td>6.3964</td>\n",
       "      <td>104.7733</td>\n",
       "      <td>6.2933</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0001</td>\n",
       "      <td>596</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>0.4062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.5783</td>\n",
       "      <td>90.0008</td>\n",
       "      <td>9.4849</td>\n",
       "      <td>89.9967</td>\n",
       "      <td>10.1107</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0004</td>\n",
       "      <td>597</td>\n",
       "      <td>33</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.9377</td>\n",
       "      <td>90.0072</td>\n",
       "      <td>6.9372</td>\n",
       "      <td>89.9880</td>\n",
       "      <td>25.0641</td>\n",
       "      <td>test</td>\n",
       "      <td>119.9857</td>\n",
       "      <td>598</td>\n",
       "      <td>194</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.1841</td>\n",
       "      <td>90.0041</td>\n",
       "      <td>8.8659</td>\n",
       "      <td>90.0009</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>test</td>\n",
       "      <td>90.0007</td>\n",
       "      <td>599</td>\n",
       "      <td>33</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>90.0029</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>90.0031</td>\n",
       "      <td>9.4956</td>\n",
       "      <td>test</td>\n",
       "      <td>89.9969</td>\n",
       "      <td>600</td>\n",
       "      <td>206</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.2812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       E  Eg  Natoms        a    alpha       b      beta        c dataset  \\\n",
       "2995 NaN NaN    80.0  24.8145  90.0002  6.3964  104.7733   6.2933    test   \n",
       "2996 NaN NaN    40.0   5.5783  90.0008  9.4849   89.9967  10.1107    test   \n",
       "2997 NaN NaN    80.0   6.9377  90.0072  6.9372   89.9880  25.0641    test   \n",
       "2998 NaN NaN    40.0   5.1841  90.0041  8.8659   90.0009   9.4956    test   \n",
       "2999 NaN NaN    80.0   9.4959  90.0029  9.4956   90.0031   9.4956    test   \n",
       "\n",
       "         gamma   id   sg   x_Al    x_Ga    x_In  \n",
       "2995   90.0001  596   12  0.000  0.5938  0.4062  \n",
       "2996   90.0004  597   33  0.125  0.0000  0.8750  \n",
       "2997  119.9857  598  194  0.000  0.2500  0.7500  \n",
       "2998   90.0007  599   33  0.625  0.0000  0.3750  \n",
       "2999   89.9969  600  206  0.375  0.3438  0.2812  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/cbartel/random-forest-using-elemental-properties\n",
    "def get_vol(a, b, c, alpha, beta, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a (float) - lattice vector 1\n",
    "        b (float) - lattice vector 2\n",
    "        c (float) - lattice vector 3\n",
    "        alpha (float) - lattice angle 1 [radians]\n",
    "        beta (float) - lattice angle 2 [radians]\n",
    "        gamma (float) - lattice angle 3 [radians]\n",
    "    Returns:\n",
    "        volume (float) of the parallelepiped unit cell\n",
    "    \"\"\"\n",
    "    alpha=alpha*np.pi/180\n",
    "    beta=beta*np.pi/180\n",
    "    gamma=gamma*np.pi/180\n",
    "    return a*b*c*np.sqrt(1 + 2*np.cos(alpha)*np.cos(beta)*np.cos(gamma)\n",
    "                           - np.cos(alpha)**2\n",
    "                           - np.cos(beta)**2\n",
    "                           - np.cos(gamma)**2)\n",
    "\n",
    "\n",
    "    \n",
    "# compute the cell volumes \n",
    "df_total['vol'] = get_vol(df_total['a'], df_total['b'], df_total['c'],\n",
    "                          df_total['alpha'], df_total['beta'], df_total['gamma'])\n",
    "#df_total[['a','b','c','alpha','beta','gamma','vol']].head()\n",
    "df_total['density']=df_total['Natoms']/df_total[\"vol\"]\n",
    "df_total['density_Al']=df_total['density']*df_total['x_Al']\n",
    "df_total['density_Ga']=df_total['density']*df_total['x_Ga']\n",
    "df_total['density_In']=df_total['density']*df_total['x_In']\n",
    "df_total['sg']=df_total['sg'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>Eg</th>\n",
       "      <th>Natoms</th>\n",
       "      <th>a</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b</th>\n",
       "      <th>beta</th>\n",
       "      <th>c</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gamma</th>\n",
       "      <th>id</th>\n",
       "      <th>sg</th>\n",
       "      <th>x_Al</th>\n",
       "      <th>x_Ga</th>\n",
       "      <th>x_In</th>\n",
       "      <th>vol</th>\n",
       "      <th>density</th>\n",
       "      <th>density_Al</th>\n",
       "      <th>density_Ga</th>\n",
       "      <th>density_In</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065788</td>\n",
       "      <td>1.490362</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.9523</td>\n",
       "      <td>90.0026</td>\n",
       "      <td>8.5513</td>\n",
       "      <td>90.0023</td>\n",
       "      <td>9.1775</td>\n",
       "      <td>train</td>\n",
       "      <td>90.0017</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>781.052081</td>\n",
       "      <td>0.102426</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222343</td>\n",
       "      <td>1.366347</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.1840</td>\n",
       "      <td>90.0186</td>\n",
       "      <td>6.1838</td>\n",
       "      <td>89.9980</td>\n",
       "      <td>23.6287</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0025</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>782.500110</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.038339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.167293</td>\n",
       "      <td>1.320101</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.7510</td>\n",
       "      <td>90.9688</td>\n",
       "      <td>5.6595</td>\n",
       "      <td>91.1228</td>\n",
       "      <td>13.9630</td>\n",
       "      <td>train</td>\n",
       "      <td>30.5185</td>\n",
       "      <td>3</td>\n",
       "      <td>227</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>391.227531</td>\n",
       "      <td>0.102242</td>\n",
       "      <td>0.083072</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196553</td>\n",
       "      <td>1.469992</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0036</td>\n",
       "      <td>89.9888</td>\n",
       "      <td>5.0034</td>\n",
       "      <td>90.0119</td>\n",
       "      <td>13.5318</td>\n",
       "      <td>train</td>\n",
       "      <td>120.0017</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>293.377334</td>\n",
       "      <td>0.102257</td>\n",
       "      <td>0.076693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049266</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.6614</td>\n",
       "      <td>89.9960</td>\n",
       "      <td>6.6612</td>\n",
       "      <td>90.0006</td>\n",
       "      <td>24.5813</td>\n",
       "      <td>train</td>\n",
       "      <td>119.9893</td>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.375</td>\n",
       "      <td>944.713843</td>\n",
       "      <td>0.084682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.031756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          E        Eg  Natoms       a    alpha       b     beta        c  \\\n",
       "0  0.065788  1.490362    80.0  9.9523  90.0026  8.5513  90.0023   9.1775   \n",
       "1  0.222343  1.366347    80.0  6.1840  90.0186  6.1838  89.9980  23.6287   \n",
       "2  0.167293  1.320101    40.0  9.7510  90.9688  5.6595  91.1228  13.9630   \n",
       "3  0.196553  1.469992    30.0  5.0036  89.9888  5.0034  90.0119  13.5318   \n",
       "4  0.049266  0.866806    80.0  6.6614  89.9960  6.6612  90.0006  24.5813   \n",
       "\n",
       "  dataset     gamma  id   sg    x_Al    x_Ga   x_In         vol   density  \\\n",
       "0   train   90.0017   1   33  0.6250  0.3750  0.000  781.052081  0.102426   \n",
       "1   train  120.0025   2  194  0.6250  0.3750  0.000  782.500110  0.102236   \n",
       "2   train   30.5185   3  227  0.8125  0.1875  0.000  391.227531  0.102242   \n",
       "3   train  120.0017   4  167  0.7500  0.0000  0.250  293.377334  0.102257   \n",
       "4   train  119.9893   5  194  0.0000  0.6250  0.375  944.713843  0.084682   \n",
       "\n",
       "   density_Al  density_Ga  density_In  \n",
       "0    0.064016    0.038410    0.000000  \n",
       "1    0.063898    0.038339    0.000000  \n",
       "2    0.083072    0.019170    0.000000  \n",
       "3    0.076693    0.000000    0.025564  \n",
       "4    0.000000    0.052926    0.031756  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pymatgen\n",
    "elem_prop=pd.concat([pd.Series(pymatgen.Element.Al.data,name='Al'), pd.Series(pymatgen.Element.Ga.data,name='Ga'),pd.Series(pymatgen.Element.In.data,name='In')], axis=1)\n",
    "keep=['Atomic mass','Atomic radius','Atomic radius calculated','Density of solid','Ionic radii','Molar volume','Thermal conductivity','Van der waals radius','Velocity of sound','X' ]\n",
    "keep_names=['a_mass','a_radius','a_radius_c','elem_density','ionic_r','molar_v','t_cond','vdw_radius','sound_v','X' ]\n",
    "#keep=['Atomic mass']\n",
    "elem_prop=elem_prop.T\n",
    "elem_prop2=elem_prop[keep]\n",
    "elem_prop2['Ionic radii']=elem_prop2['Ionic radii'].apply(lambda x: (list(x.values()))[0])\n",
    "for c in ['Molar volume','Density of solid','Thermal conductivity','Velocity of sound']:\n",
    "    elem_prop2[c]=elem_prop2[c].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "for c in   elem_prop2.columns:\n",
    "     elem_prop2[c]=elem_prop2[c].astype(\"float32\")\n",
    "\n",
    "elem_prop2.rename(columns={old:new for (old,new) in zip(keep,keep_names)},inplace=True)\n",
    "\n",
    "print(elem_prop2)\n",
    "\n",
    "def avg_prop(x_Al, x_Ga, x_In, prop):\n",
    "    return elem_prop2.loc['Al',prop]*x_Al+elem_prop2.loc['Ga',prop]*x_Ga+elem_prop2.loc['In',prop]*x_In\n",
    "\n",
    "properties=elem_prop2.columns\n",
    "\n",
    "for prop in properties:\n",
    "    df_total['_'.join(['avg', prop])] = avg_prop(df_train['x_Al'], df_train['x_Ga'],df_train['x_In'],prop)\n",
    "    \n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encoding of cat features\n",
    "import sys \n",
    "sys.path.append(\"../kaggle_varie\")\n",
    "from  varie import *\n",
    "cols_to_enc=[\"sg\"]\n",
    "\n",
    "#binary encoder\n",
    "#enc=bin_enc(df_total,cols_to_enc,verbose=2,copy=True,drop_original=True,ordinal_only=False)\n",
    "#one-hot encoder\n",
    "enc=pd.get_dummies(df_total,columns=cols_to_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_fct(model,params,df,y_col,n_iter=20,cv=4,drop_col=[],verbose=2):\n",
    "    \n",
    "    X_train=df.drop(y_col+drop_col,axis=1).values\n",
    "    grids=[]\n",
    "    for y in y_col:\n",
    "        print(y)\n",
    "        y_train=df[y].values\n",
    "        print(X_train.shape,y_train.shape)\n",
    "\n",
    "        grid=RandomizedSearchCV(model,param_distributions=params, n_iter=n_iter,cv=cv,verbose=verbose,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "        grid.fit(X_train,y_train)\n",
    "        grids.append(grid)\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adb\n",
      "E\n",
      "(2400, 21) (2400,)\n",
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] n_estimators=235, learning_rate=0.07109792420131451 .............\n",
      "[CV]  n_estimators=235, learning_rate=0.07109792420131451, total=   0.4s\n",
      "[CV] n_estimators=235, learning_rate=0.07109792420131451 .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=235, learning_rate=0.07109792420131451, total=   0.5s\n",
      "[CV] n_estimators=235, learning_rate=0.07109792420131451 .............\n",
      "[CV]  n_estimators=235, learning_rate=0.07109792420131451, total=   0.8s\n",
      "[CV] n_estimators=235, learning_rate=0.07109792420131451 .............\n",
      "[CV]  n_estimators=235, learning_rate=0.07109792420131451, total=   0.8s\n",
      "[CV] n_estimators=153, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=153, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=153, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=153, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=153, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=153, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=153, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=153, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=366, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=366, learning_rate=0.0024020430925079488, total=   1.6s\n",
      "[CV] n_estimators=366, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=366, learning_rate=0.0024020430925079488, total=   1.6s\n",
      "[CV] n_estimators=366, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=366, learning_rate=0.0024020430925079488, total=   1.6s\n",
      "[CV] n_estimators=366, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=366, learning_rate=0.0024020430925079488, total=   1.6s\n",
      "[CV] n_estimators=342, learning_rate=0.0005178845428242239 ...........\n",
      "[CV]  n_estimators=342, learning_rate=0.0005178845428242239, total=   1.5s\n",
      "[CV] n_estimators=342, learning_rate=0.0005178845428242239 ...........\n",
      "[CV]  n_estimators=342, learning_rate=0.0005178845428242239, total=   1.5s\n",
      "[CV] n_estimators=342, learning_rate=0.0005178845428242239 ...........\n",
      "[CV]  n_estimators=342, learning_rate=0.0005178845428242239, total=   1.5s\n",
      "[CV] n_estimators=342, learning_rate=0.0005178845428242239 ...........\n",
      "[CV]  n_estimators=342, learning_rate=0.0005178845428242239, total=   1.5s\n",
      "[CV] n_estimators=174, learning_rate=0.22073327553370037 .............\n",
      "[CV]  n_estimators=174, learning_rate=0.22073327553370037, total=   0.1s\n",
      "[CV] n_estimators=174, learning_rate=0.22073327553370037 .............\n",
      "[CV]  n_estimators=174, learning_rate=0.22073327553370037, total=   0.2s\n",
      "[CV] n_estimators=174, learning_rate=0.22073327553370037 .............\n",
      "[CV]  n_estimators=174, learning_rate=0.22073327553370037, total=   0.4s\n",
      "[CV] n_estimators=174, learning_rate=0.22073327553370037 .............\n",
      "[CV]  n_estimators=174, learning_rate=0.22073327553370037, total=   0.4s\n",
      "[CV] n_estimators=53, learning_rate=0.008143156949743957 .............\n",
      "[CV]  n_estimators=53, learning_rate=0.008143156949743957, total=   0.2s\n",
      "[CV] n_estimators=53, learning_rate=0.008143156949743957 .............\n",
      "[CV]  n_estimators=53, learning_rate=0.008143156949743957, total=   0.2s\n",
      "[CV] n_estimators=53, learning_rate=0.008143156949743957 .............\n",
      "[CV]  n_estimators=53, learning_rate=0.008143156949743957, total=   0.2s\n",
      "[CV] n_estimators=53, learning_rate=0.008143156949743957 .............\n",
      "[CV]  n_estimators=53, learning_rate=0.008143156949743957, total=   0.2s\n",
      "[CV] n_estimators=288, learning_rate=0.0001470921416137209 ...........\n",
      "[CV]  n_estimators=288, learning_rate=0.0001470921416137209, total=   1.3s\n",
      "[CV] n_estimators=288, learning_rate=0.0001470921416137209 ...........\n",
      "[CV]  n_estimators=288, learning_rate=0.0001470921416137209, total=   1.3s\n",
      "[CV] n_estimators=288, learning_rate=0.0001470921416137209 ...........\n",
      "[CV]  n_estimators=288, learning_rate=0.0001470921416137209, total=   1.3s\n",
      "[CV] n_estimators=288, learning_rate=0.0001470921416137209 ...........\n",
      "[CV]  n_estimators=288, learning_rate=0.0001470921416137209, total=   1.3s\n",
      "[CV] n_estimators=280, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=280, learning_rate=0.040475148528734475, total=   0.8s\n",
      "[CV] n_estimators=280, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=280, learning_rate=0.040475148528734475, total=   0.8s\n",
      "[CV] n_estimators=280, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=280, learning_rate=0.040475148528734475, total=   1.0s\n",
      "[CV] n_estimators=280, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=280, learning_rate=0.040475148528734475, total=   1.0s\n",
      "[CV] n_estimators=163, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=163, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=163, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=163, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=163, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=163, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=163, learning_rate=0.0022317174041792715 ...........\n",
      "[CV]  n_estimators=163, learning_rate=0.0022317174041792715, total=   0.7s\n",
      "[CV] n_estimators=114, learning_rate=0.6504821458404746 ..............\n",
      "[CV]  n_estimators=114, learning_rate=0.6504821458404746, total=   0.1s\n",
      "[CV] n_estimators=114, learning_rate=0.6504821458404746 ..............\n",
      "[CV]  n_estimators=114, learning_rate=0.6504821458404746, total=   0.1s\n",
      "[CV] n_estimators=114, learning_rate=0.6504821458404746 ..............\n",
      "[CV]  n_estimators=114, learning_rate=0.6504821458404746, total=   0.2s\n",
      "[CV] n_estimators=114, learning_rate=0.6504821458404746 ..............\n",
      "[CV]  n_estimators=114, learning_rate=0.6504821458404746, total=   0.2s\n",
      "[CV] n_estimators=290, learning_rate=0.0008950796979703272 ...........\n",
      "[CV]  n_estimators=290, learning_rate=0.0008950796979703272, total=   1.3s\n",
      "[CV] n_estimators=290, learning_rate=0.0008950796979703272 ...........\n",
      "[CV]  n_estimators=290, learning_rate=0.0008950796979703272, total=   1.3s\n",
      "[CV] n_estimators=290, learning_rate=0.0008950796979703272 ...........\n",
      "[CV]  n_estimators=290, learning_rate=0.0008950796979703272, total=   1.3s\n",
      "[CV] n_estimators=290, learning_rate=0.0008950796979703272 ...........\n",
      "[CV]  n_estimators=290, learning_rate=0.0008950796979703272, total=   1.3s\n",
      "[CV] n_estimators=293, learning_rate=0.009966548282332144 ............\n",
      "[CV]  n_estimators=293, learning_rate=0.009966548282332144, total=   1.3s\n",
      "[CV] n_estimators=293, learning_rate=0.009966548282332144 ............\n",
      "[CV]  n_estimators=293, learning_rate=0.009966548282332144, total=   1.3s\n",
      "[CV] n_estimators=293, learning_rate=0.009966548282332144 ............\n",
      "[CV]  n_estimators=293, learning_rate=0.009966548282332144, total=   1.3s\n",
      "[CV] n_estimators=293, learning_rate=0.009966548282332144 ............\n",
      "[CV]  n_estimators=293, learning_rate=0.009966548282332144, total=   1.3s\n",
      "[CV] n_estimators=76, learning_rate=0.04471621028864842 ..............\n",
      "[CV]  n_estimators=76, learning_rate=0.04471621028864842, total=   0.3s\n",
      "[CV] n_estimators=76, learning_rate=0.04471621028864842 ..............\n",
      "[CV]  n_estimators=76, learning_rate=0.04471621028864842, total=   0.3s\n",
      "[CV] n_estimators=76, learning_rate=0.04471621028864842 ..............\n",
      "[CV]  n_estimators=76, learning_rate=0.04471621028864842, total=   0.3s\n",
      "[CV] n_estimators=76, learning_rate=0.04471621028864842 ..............\n",
      "[CV]  n_estimators=76, learning_rate=0.04471621028864842, total=   0.3s\n",
      "[CV] n_estimators=172, learning_rate=0.42951974771264356 .............\n",
      "[CV]  n_estimators=172, learning_rate=0.42951974771264356, total=   0.1s\n",
      "[CV] n_estimators=172, learning_rate=0.42951974771264356 .............\n",
      "[CV]  n_estimators=172, learning_rate=0.42951974771264356, total=   0.1s\n",
      "[CV] n_estimators=172, learning_rate=0.42951974771264356 .............\n",
      "[CV]  n_estimators=172, learning_rate=0.42951974771264356, total=   0.4s\n",
      "[CV] n_estimators=172, learning_rate=0.42951974771264356 .............\n",
      "[CV]  n_estimators=172, learning_rate=0.42951974771264356, total=   0.5s\n",
      "[CV] n_estimators=153, learning_rate=0.004330186866161943 ............\n",
      "[CV]  n_estimators=153, learning_rate=0.004330186866161943, total=   0.8s\n",
      "[CV] n_estimators=153, learning_rate=0.004330186866161943 ............\n",
      "[CV]  n_estimators=153, learning_rate=0.004330186866161943, total=   0.7s\n",
      "[CV] n_estimators=153, learning_rate=0.004330186866161943 ............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=153, learning_rate=0.004330186866161943, total=   0.7s\n",
      "[CV] n_estimators=153, learning_rate=0.004330186866161943 ............\n",
      "[CV]  n_estimators=153, learning_rate=0.004330186866161943, total=   0.7s\n",
      "[CV] n_estimators=294, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=294, learning_rate=0.003071022281488235, total=   1.3s\n",
      "[CV] n_estimators=294, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=294, learning_rate=0.003071022281488235, total=   1.4s\n",
      "[CV] n_estimators=294, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=294, learning_rate=0.003071022281488235, total=   1.3s\n",
      "[CV] n_estimators=294, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=294, learning_rate=0.003071022281488235, total=   1.4s\n",
      "[CV] n_estimators=323, learning_rate=0.5586995159146345 ..............\n",
      "[CV]  n_estimators=323, learning_rate=0.5586995159146345, total=   0.1s\n",
      "[CV] n_estimators=323, learning_rate=0.5586995159146345 ..............\n",
      "[CV]  n_estimators=323, learning_rate=0.5586995159146345, total=   0.1s\n",
      "[CV] n_estimators=323, learning_rate=0.5586995159146345 ..............\n",
      "[CV]  n_estimators=323, learning_rate=0.5586995159146345, total=   0.6s\n",
      "[CV] n_estimators=323, learning_rate=0.5586995159146345 ..............\n",
      "[CV]  n_estimators=323, learning_rate=0.5586995159146345, total=   0.6s\n",
      "[CV] n_estimators=392, learning_rate=0.40209670167580397 .............\n",
      "[CV]  n_estimators=392, learning_rate=0.40209670167580397, total=   0.1s\n",
      "[CV] n_estimators=392, learning_rate=0.40209670167580397 .............\n",
      "[CV]  n_estimators=392, learning_rate=0.40209670167580397, total=   0.1s\n",
      "[CV] n_estimators=392, learning_rate=0.40209670167580397 .............\n",
      "[CV]  n_estimators=392, learning_rate=0.40209670167580397, total=   0.7s\n",
      "[CV] n_estimators=392, learning_rate=0.40209670167580397 .............\n",
      "[CV]  n_estimators=392, learning_rate=0.40209670167580397, total=   0.7s\n",
      "[CV] n_estimators=125, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=125, learning_rate=0.009723521346457912, total=   0.5s\n",
      "[CV] n_estimators=125, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=125, learning_rate=0.009723521346457912, total=   0.5s\n",
      "[CV] n_estimators=125, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=125, learning_rate=0.009723521346457912, total=   0.5s\n",
      "[CV] n_estimators=125, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=125, learning_rate=0.009723521346457912, total=   0.5s\n",
      "[CV] n_estimators=42, learning_rate=0.004330186866161943 .............\n",
      "[CV]  n_estimators=42, learning_rate=0.004330186866161943, total=   0.2s\n",
      "[CV] n_estimators=42, learning_rate=0.004330186866161943 .............\n",
      "[CV]  n_estimators=42, learning_rate=0.004330186866161943, total=   0.2s\n",
      "[CV] n_estimators=42, learning_rate=0.004330186866161943 .............\n",
      "[CV]  n_estimators=42, learning_rate=0.004330186866161943, total=   0.2s\n",
      "[CV] n_estimators=42, learning_rate=0.004330186866161943 .............\n",
      "[CV]  n_estimators=42, learning_rate=0.004330186866161943, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eg\n",
      "(2400, 21) (2400,)\n",
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "[CV] n_estimators=298, learning_rate=0.013751899574428378 ............\n",
      "[CV]  n_estimators=298, learning_rate=0.013751899574428378, total=   1.3s\n",
      "[CV] n_estimators=298, learning_rate=0.013751899574428378 ............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=298, learning_rate=0.013751899574428378, total=   1.3s\n",
      "[CV] n_estimators=298, learning_rate=0.013751899574428378 ............\n",
      "[CV]  n_estimators=298, learning_rate=0.013751899574428378, total=   1.3s\n",
      "[CV] n_estimators=298, learning_rate=0.013751899574428378 ............\n",
      "[CV]  n_estimators=298, learning_rate=0.013751899574428378, total=   1.3s\n",
      "[CV] n_estimators=114, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=114, learning_rate=0.040475148528734475, total=   0.5s\n",
      "[CV] n_estimators=114, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=114, learning_rate=0.040475148528734475, total=   0.5s\n",
      "[CV] n_estimators=114, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=114, learning_rate=0.040475148528734475, total=   0.5s\n",
      "[CV] n_estimators=114, learning_rate=0.040475148528734475 ............\n",
      "[CV]  n_estimators=114, learning_rate=0.040475148528734475, total=   0.5s\n",
      "[CV] n_estimators=80, learning_rate=0.7410505900629079 ...............\n",
      "[CV]  n_estimators=80, learning_rate=0.7410505900629079, total=   0.2s\n",
      "[CV] n_estimators=80, learning_rate=0.7410505900629079 ...............\n",
      "[CV]  n_estimators=80, learning_rate=0.7410505900629079, total=   0.2s\n",
      "[CV] n_estimators=80, learning_rate=0.7410505900629079 ...............\n",
      "[CV]  n_estimators=80, learning_rate=0.7410505900629079, total=   0.2s\n",
      "[CV] n_estimators=80, learning_rate=0.7410505900629079 ...............\n",
      "[CV]  n_estimators=80, learning_rate=0.7410505900629079, total=   0.2s\n",
      "[CV] n_estimators=242, learning_rate=0.1938040497096698 ..............\n",
      "[CV]  n_estimators=242, learning_rate=0.1938040497096698, total=   0.7s\n",
      "[CV] n_estimators=242, learning_rate=0.1938040497096698 ..............\n",
      "[CV]  n_estimators=242, learning_rate=0.1938040497096698, total=   0.7s\n",
      "[CV] n_estimators=242, learning_rate=0.1938040497096698 ..............\n",
      "[CV]  n_estimators=242, learning_rate=0.1938040497096698, total=   0.7s\n",
      "[CV] n_estimators=242, learning_rate=0.1938040497096698 ..............\n",
      "[CV]  n_estimators=242, learning_rate=0.1938040497096698, total=   0.7s\n",
      "[CV] n_estimators=60, learning_rate=0.0017900840718853646 ............\n",
      "[CV]  n_estimators=60, learning_rate=0.0017900840718853646, total=   0.3s\n",
      "[CV] n_estimators=60, learning_rate=0.0017900840718853646 ............\n",
      "[CV]  n_estimators=60, learning_rate=0.0017900840718853646, total=   0.3s\n",
      "[CV] n_estimators=60, learning_rate=0.0017900840718853646 ............\n",
      "[CV]  n_estimators=60, learning_rate=0.0017900840718853646, total=   0.3s\n",
      "[CV] n_estimators=60, learning_rate=0.0017900840718853646 ............\n",
      "[CV]  n_estimators=60, learning_rate=0.0017900840718853646, total=   0.3s\n",
      "[CV] n_estimators=379, learning_rate=0.2248967630369945 ..............\n",
      "[CV]  n_estimators=379, learning_rate=0.2248967630369945, total=   0.9s\n",
      "[CV] n_estimators=379, learning_rate=0.2248967630369945 ..............\n",
      "[CV]  n_estimators=379, learning_rate=0.2248967630369945, total=   1.0s\n",
      "[CV] n_estimators=379, learning_rate=0.2248967630369945 ..............\n",
      "[CV]  n_estimators=379, learning_rate=0.2248967630369945, total=   0.9s\n",
      "[CV] n_estimators=379, learning_rate=0.2248967630369945 ..............\n",
      "[CV]  n_estimators=379, learning_rate=0.2248967630369945, total=   1.0s\n",
      "[CV] n_estimators=214, learning_rate=0.009764546946093699 ............\n",
      "[CV]  n_estimators=214, learning_rate=0.009764546946093699, total=   0.9s\n",
      "[CV] n_estimators=214, learning_rate=0.009764546946093699 ............\n",
      "[CV]  n_estimators=214, learning_rate=0.009764546946093699, total=   0.9s\n",
      "[CV] n_estimators=214, learning_rate=0.009764546946093699 ............\n",
      "[CV]  n_estimators=214, learning_rate=0.009764546946093699, total=   0.9s\n",
      "[CV] n_estimators=214, learning_rate=0.009764546946093699 ............\n",
      "[CV]  n_estimators=214, learning_rate=0.009764546946093699, total=   0.9s\n",
      "[CV] n_estimators=121, learning_rate=0.0007999749628021702 ...........\n",
      "[CV]  n_estimators=121, learning_rate=0.0007999749628021702, total=   0.5s\n",
      "[CV] n_estimators=121, learning_rate=0.0007999749628021702 ...........\n",
      "[CV]  n_estimators=121, learning_rate=0.0007999749628021702, total=   0.5s\n",
      "[CV] n_estimators=121, learning_rate=0.0007999749628021702 ...........\n",
      "[CV]  n_estimators=121, learning_rate=0.0007999749628021702, total=   0.5s\n",
      "[CV] n_estimators=121, learning_rate=0.0007999749628021702 ...........\n",
      "[CV]  n_estimators=121, learning_rate=0.0007999749628021702, total=   0.5s\n",
      "[CV] n_estimators=95, learning_rate=0.00019312976160926619 ...........\n",
      "[CV]  n_estimators=95, learning_rate=0.00019312976160926619, total=   0.5s\n",
      "[CV] n_estimators=95, learning_rate=0.00019312976160926619 ...........\n",
      "[CV]  n_estimators=95, learning_rate=0.00019312976160926619, total=   0.4s\n",
      "[CV] n_estimators=95, learning_rate=0.00019312976160926619 ...........\n",
      "[CV]  n_estimators=95, learning_rate=0.00019312976160926619, total=   0.4s\n",
      "[CV] n_estimators=95, learning_rate=0.00019312976160926619 ...........\n",
      "[CV]  n_estimators=95, learning_rate=0.00019312976160926619, total=   0.4s\n",
      "[CV] n_estimators=103, learning_rate=0.0030484958580430386 ...........\n",
      "[CV]  n_estimators=103, learning_rate=0.0030484958580430386, total=   0.5s\n",
      "[CV] n_estimators=103, learning_rate=0.0030484958580430386 ...........\n",
      "[CV]  n_estimators=103, learning_rate=0.0030484958580430386, total=   0.5s\n",
      "[CV] n_estimators=103, learning_rate=0.0030484958580430386 ...........\n",
      "[CV]  n_estimators=103, learning_rate=0.0030484958580430386, total=   0.5s\n",
      "[CV] n_estimators=103, learning_rate=0.0030484958580430386 ...........\n",
      "[CV]  n_estimators=103, learning_rate=0.0030484958580430386, total=   0.5s\n",
      "[CV] n_estimators=373, learning_rate=0.2741045459263702 ..............\n",
      "[CV]  n_estimators=373, learning_rate=0.2741045459263702, total=   0.8s\n",
      "[CV] n_estimators=373, learning_rate=0.2741045459263702 ..............\n",
      "[CV]  n_estimators=373, learning_rate=0.2741045459263702, total=   1.0s\n",
      "[CV] n_estimators=373, learning_rate=0.2741045459263702 ..............\n",
      "[CV]  n_estimators=373, learning_rate=0.2741045459263702, total=   0.9s\n",
      "[CV] n_estimators=373, learning_rate=0.2741045459263702 ..............\n",
      "[CV]  n_estimators=373, learning_rate=0.2741045459263702, total=   0.9s\n",
      "[CV] n_estimators=113, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=113, learning_rate=0.003071022281488235, total=   0.5s\n",
      "[CV] n_estimators=113, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=113, learning_rate=0.003071022281488235, total=   0.5s\n",
      "[CV] n_estimators=113, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=113, learning_rate=0.003071022281488235, total=   0.5s\n",
      "[CV] n_estimators=113, learning_rate=0.003071022281488235 ............\n",
      "[CV]  n_estimators=113, learning_rate=0.003071022281488235, total=   0.5s\n",
      "[CV] n_estimators=182, learning_rate=0.002142473127374868 ............\n",
      "[CV]  n_estimators=182, learning_rate=0.002142473127374868, total=   0.8s\n",
      "[CV] n_estimators=182, learning_rate=0.002142473127374868 ............\n",
      "[CV]  n_estimators=182, learning_rate=0.002142473127374868, total=   0.8s\n",
      "[CV] n_estimators=182, learning_rate=0.002142473127374868 ............\n",
      "[CV]  n_estimators=182, learning_rate=0.002142473127374868, total=   0.8s\n",
      "[CV] n_estimators=182, learning_rate=0.002142473127374868 ............\n",
      "[CV]  n_estimators=182, learning_rate=0.002142473127374868, total=   0.8s\n",
      "[CV] n_estimators=313, learning_rate=0.18437005819731586 .............\n",
      "[CV]  n_estimators=313, learning_rate=0.18437005819731586, total=   0.8s\n",
      "[CV] n_estimators=313, learning_rate=0.18437005819731586 .............\n",
      "[CV]  n_estimators=313, learning_rate=0.18437005819731586, total=   0.9s\n",
      "[CV] n_estimators=313, learning_rate=0.18437005819731586 .............\n",
      "[CV]  n_estimators=313, learning_rate=0.18437005819731586, total=   0.8s\n",
      "[CV] n_estimators=313, learning_rate=0.18437005819731586 .............\n",
      "[CV]  n_estimators=313, learning_rate=0.18437005819731586, total=   0.8s\n",
      "[CV] n_estimators=387, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=387, learning_rate=0.009723521346457912, total=   1.7s\n",
      "[CV] n_estimators=387, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=387, learning_rate=0.009723521346457912, total=   1.7s\n",
      "[CV] n_estimators=387, learning_rate=0.009723521346457912 ............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=387, learning_rate=0.009723521346457912, total=   1.7s\n",
      "[CV] n_estimators=387, learning_rate=0.009723521346457912 ............\n",
      "[CV]  n_estimators=387, learning_rate=0.009723521346457912, total=   1.7s\n",
      "[CV] n_estimators=355, learning_rate=0.001015141334160562 ............\n",
      "[CV]  n_estimators=355, learning_rate=0.001015141334160562, total=   1.6s\n",
      "[CV] n_estimators=355, learning_rate=0.001015141334160562 ............\n",
      "[CV]  n_estimators=355, learning_rate=0.001015141334160562, total=   1.6s\n",
      "[CV] n_estimators=355, learning_rate=0.001015141334160562 ............\n",
      "[CV]  n_estimators=355, learning_rate=0.001015141334160562, total=   1.7s\n",
      "[CV] n_estimators=355, learning_rate=0.001015141334160562 ............\n",
      "[CV]  n_estimators=355, learning_rate=0.001015141334160562, total=   1.7s\n",
      "[CV] n_estimators=259, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=259, learning_rate=0.0024020430925079488, total=   1.2s\n",
      "[CV] n_estimators=259, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=259, learning_rate=0.0024020430925079488, total=   1.2s\n",
      "[CV] n_estimators=259, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=259, learning_rate=0.0024020430925079488, total=   1.2s\n",
      "[CV] n_estimators=259, learning_rate=0.0024020430925079488 ...........\n",
      "[CV]  n_estimators=259, learning_rate=0.0024020430925079488, total=   1.2s\n",
      "[CV] n_estimators=365, learning_rate=0.34873233304496787 .............\n",
      "[CV]  n_estimators=365, learning_rate=0.34873233304496787, total=   0.8s\n",
      "[CV] n_estimators=365, learning_rate=0.34873233304496787 .............\n",
      "[CV]  n_estimators=365, learning_rate=0.34873233304496787, total=   0.9s\n",
      "[CV] n_estimators=365, learning_rate=0.34873233304496787 .............\n",
      "[CV]  n_estimators=365, learning_rate=0.34873233304496787, total=   0.8s\n",
      "[CV] n_estimators=365, learning_rate=0.34873233304496787 .............\n",
      "[CV]  n_estimators=365, learning_rate=0.34873233304496787, total=   0.8s\n",
      "[CV] n_estimators=291, learning_rate=0.2554977686845243 ..............\n",
      "[CV]  n_estimators=291, learning_rate=0.2554977686845243, total=   0.7s\n",
      "[CV] n_estimators=291, learning_rate=0.2554977686845243 ..............\n",
      "[CV]  n_estimators=291, learning_rate=0.2554977686845243, total=   0.8s\n",
      "[CV] n_estimators=291, learning_rate=0.2554977686845243 ..............\n",
      "[CV]  n_estimators=291, learning_rate=0.2554977686845243, total=   0.7s\n",
      "[CV] n_estimators=291, learning_rate=0.2554977686845243 ..............\n",
      "[CV]  n_estimators=291, learning_rate=0.2554977686845243, total=   0.7s\n",
      "[CV] n_estimators=50, learning_rate=0.002142473127374868 .............\n",
      "[CV]  n_estimators=50, learning_rate=0.002142473127374868, total=   0.2s\n",
      "[CV] n_estimators=50, learning_rate=0.002142473127374868 .............\n",
      "[CV]  n_estimators=50, learning_rate=0.002142473127374868, total=   0.2s\n",
      "[CV] n_estimators=50, learning_rate=0.002142473127374868 .............\n",
      "[CV]  n_estimators=50, learning_rate=0.002142473127374868, total=   0.2s\n",
      "[CV] n_estimators=50, learning_rate=0.002142473127374868 .............\n",
      "[CV]  n_estimators=50, learning_rate=0.002142473127374868, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "#grid search for random forest\n",
    "import scipy\n",
    "from  sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import *\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import  GradientBoostingRegressor, RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from sklearn.kernel_approximation import Nystroem\n",
    "#from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "from xgboost import XGBRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "def lognuniform(low=0, high=1, size=None, base=10):\n",
    "    return np.power(base, np.random.uniform(low, high, size))\n",
    "\n",
    "y_col=[\"E\",\"Eg\"]\n",
    "drop_col=[\"id\",\"dataset\"]\n",
    "df_total_train_eval=enc[df_total.dataset=='train']\n",
    "df_total_test=enc[df_total.dataset=='test']\n",
    "\n",
    "X_train=df_total_train_eval.drop(y_col+drop_col,axis=1).values\n",
    "X_test=df_total_test.drop(y_col+drop_col,axis=1).values\n",
    "\n",
    "models={\n",
    "    \n",
    "#    'knn':\n",
    " #          (KNeighborsRegressor(),\n",
    "  #          {'n_neighbors':scipy.stats.randint(1,100)}),\n",
    "    \n",
    "#    'svr':\n",
    " #          (SVR(verbose=False),\n",
    "  #          {'C':lognuniform(low=-4,high=4,base=10,size=100),\n",
    "   #             'epsilon':lognuniform(low=-2,high=0,base=10,size=100)}),\n",
    "\n",
    "#    'rf':\n",
    "#           (ensemble.RandomForestRegressor(verbose=False),\n",
    "#            {\"max_depth\": scipy.stats.randint(1,100), \n",
    "#             'n_estimators': scipy.stats.randint(1,400),\n",
    "#             'max_features':('log2','sqrt','auto')}),\n",
    "    \n",
    "#    'cb':\n",
    "#           (CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE',verbose=False),\n",
    "#            {\"depth\": scipy.stats.randint(1,5), \n",
    "#             'iterations': scipy.stats.randint(1000,2000),\n",
    "#             'learning_rate':lognuniform(low=-2,high=-1,base=10,size=100)}),\n",
    "    \n",
    "#    'mlp': \n",
    "#           (MLPRegressor((80, 10), early_stopping=False),\n",
    "#             {'hidden_layer_sizes':scipy.stats.randint(1,100),\n",
    "#              'alpha':lognuniform(low=-5,high=-1,base=10,size=100)}),\n",
    "             \n",
    " #    'gb':\n",
    " #          (GradientBoostingRegressor(n_estimators=100),\n",
    " #           {'learning_rate':lognuniform(low=-3,high=-1,base=10,size=100), \n",
    " #            'n_estimators': scipy.stats.randint(1,300)}),\n",
    "    \n",
    "#    'lasso':\n",
    "#            (Lasso(),\n",
    "#            {'alpha':lognuniform(low=-4,high=4,base=10,size=100)}),  \n",
    "\n",
    "#    'ridge':\n",
    "#            (Ridge(),\n",
    "#            {'alpha':lognuniform(low=-4,high=4,base=10,size=100)}),\n",
    "    \n",
    " #   'xgb':\n",
    "  #      (XGBRegressor(),\n",
    "   #      {'max_depth':scipy.stats.randint(1,100), \n",
    "    #      'learning_rate':lognuniform(low=-4,high=-0.5,base=10,size=100), \n",
    "     #     'n_estimators':scipy.stats.randint(1,400)}),\n",
    "    \n",
    " #does not install    \n",
    " #   'gbm' :\n",
    " #       (lgb.LGBMRegressor(objective='regression'),\n",
    " #           {'num_leaves':scipy.stats.randint(1,200), \n",
    " #         'learning_rate':lognuniform(low=-4,high=-0.5,base=10,size=100), \n",
    " #         'n_estimators':scipy.stats.randint(1,400)}),\n",
    "    'adb' :\n",
    "        (AdaBoostRegressor(loss=\"square\"),\n",
    "            {'learning_rate':lognuniform(low=-4,high=-0.1,base=10,size=100), \n",
    "             'n_estimators':scipy.stats.randint(1,400)}),         \n",
    "\n",
    "    \n",
    "       }\n",
    " \n",
    "    \n",
    "\n",
    "#results={}\n",
    "for (tag,model) in  models.items():\n",
    "    print(tag)\n",
    "    results[tag]=grid_search_fct(model[0],model[1],df_total_train_eval,y_col,n_iter=20,cv=4,drop_col=drop_col)\n",
    "\n",
    "\n",
    "    \n",
    "    #grid=RandomizedSearchCV(model[0],param_distributions=params, n_iter=20,cv=4,verbose=2,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "                        \n",
    "    #grid.fit(X_train,y_train)\n",
    "    #grids.append(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso\n",
      "{'alpha': 0.0027387754984453234}\n",
      "{'alpha': 0.00016338531868553138}\n",
      "0.08074507599068162\n",
      "adb\n",
      "{'n_estimators': 235, 'learning_rate': 0.07109792420131451}\n",
      "{'n_estimators': 114, 'learning_rate': 0.040475148528734475}\n",
      "0.0796619233301889\n",
      "gb\n",
      "{'n_estimators': 276, 'learning_rate': 0.026887651632063816}\n",
      "{'n_estimators': 136, 'learning_rate': 0.08546898120651171}\n",
      "0.06106657593199877\n",
      "knn\n",
      "{'n_neighbors': 8}\n",
      "{'n_neighbors': 8}\n",
      "0.08431301064116939\n",
      "rf\n",
      "{'max_depth': 11, 'n_estimators': 98, 'max_features': 'sqrt'}\n",
      "{'max_depth': 9, 'n_estimators': 190, 'max_features': 'log2'}\n",
      "0.06283712300468994\n",
      "xgb\n",
      "{'max_depth': 45, 'n_estimators': 85, 'learning_rate': 0.06559377246367769}\n",
      "{'max_depth': 26, 'n_estimators': 149, 'learning_rate': 0.027448151770068534}\n",
      "0.06903262121078503\n",
      "svr\n",
      "{'C': 3.6229030858475677, 'epsilon': 0.03911398169420586}\n",
      "{'C': 2.296995560428917, 'epsilon': 0.0164353456239699}\n",
      "0.09618054076145428\n",
      "cb\n",
      "{'iterations': 1734, 'learning_rate': 0.016556953835177485, 'depth': 4}\n",
      "{'iterations': 1679, 'learning_rate': 0.016150962002772756, 'depth': 3}\n",
      "0.05918324191458124\n",
      "ridge\n",
      "{'alpha': 0.0001910584569184627}\n",
      "{'alpha': 0.001646071201684565}\n",
      "0.06672137838345311\n",
      "mlp\n",
      "{'alpha': 1.9389857941542882e-05, 'hidden_layer_sizes': 93}\n",
      "{'alpha': 0.030754900461379055, 'hidden_layer_sizes': 87}\n",
      "0.175743292835698\n"
     ]
    }
   ],
   "source": [
    "#best model and its performance\n",
    "\n",
    "for tag,grids in results.items():\n",
    "    print(tag)\n",
    "    for grid in grids:\n",
    "        print(grid.best_params_)\n",
    "    print((np.sqrt(-grids[0].best_score_)+np.sqrt(-grids[1].best_score_))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners1=[grid[0].best_estimator_ for grid in results.values()]\n",
    "learners2=[grid[1].best_estimator_ for grid in results.values()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#importance of each feature\n",
    "non_features = y_col+drop_col\n",
    "features = [col for col in list(df_total_train_eval) if col not in non_features]\n",
    "importances_E =  grids[0].best_estimator_.feature_importances_\n",
    "\n",
    "assert(len(importances_E)==len(features))\n",
    "\n",
    "descending_indices_E = np.argsort(importances_E)[::-1]\n",
    "sorted_importances_E = [importances_E[idx] for idx in descending_indices_E]\n",
    "sorted_features_E = [features[idx] for idx in descending_indices_E]\n",
    "print('most important feature for formation energy is %s' % sorted_features_E[0])\n",
    "for f, i in zip(sorted_features_E,sorted_importances_E):\n",
    "    print('E',f,i)\n",
    "    \n",
    "importances_Eg =  grids[1].best_estimator_.feature_importances_\n",
    "descending_indices_Eg = np.argsort(importances_Eg)[::-1]\n",
    "sorted_importances_Eg = [importances_Eg[idx] for idx in descending_indices_Eg]\n",
    "sorted_features_Eg = [features[idx] for idx in descending_indices_Eg]\n",
    "print('most important feature for band gap is %s' % sorted_features_Eg[0])\n",
    "\n",
    "\n",
    "    \n",
    "for f, i in zip(sorted_features_Eg,sorted_importances_Eg):\n",
    "    print('Eg',f,i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#write to csv\n",
    "%load_ext autoreload\n",
    "%aimport varie\n",
    "%autoreload 2\n",
    "#I use a different model for E and Eg\n",
    "varie.make_csv2(df_total_train_eval,pd.DataFrame(),df_total_test,\n",
    "#         (ensemble.RandomForestRegressor(max_depth= 11, max_features='log2', n_estimators= 55),\n",
    "#          ensemble.RandomForestRegressor(max_depth= 9, max_features='sqrt', n_estimators= 220)),\n",
    "            (grids[0].best_estimator_,grids[1].best_estimator_),\n",
    "         y_col,'rf2c.csv',drop=drop_col,columns=['id','E','Eg'],\n",
    "         new_column_names=['id','formation_energy_ev_natom' ,'bandgap_energy_ev'],change_col_names=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from  catboost import CatBoostRegressor\n",
    "def runCatBoost(x_train, y_train,x_test, y_test,test,depth):\n",
    "    model=CatBoostRegressor(iterations=1200,\n",
    "                            learning_rate=0.03,\n",
    "                            depth=depth,\n",
    "                            loss_function='RMSE',\n",
    "                            eval_metric='RMSE',\n",
    "                            random_seed=99,\n",
    "                            od_type='Iter',\n",
    "                            od_wait=50)\n",
    "    model.fit(x_train, y_train, eval_set=(x_test, y_test), use_best_model=True, verbose=False)\n",
    "    y_pred_train=model.predict(x_test)\n",
    "    rmsle_result = rmsle(np.exp(y_pred_train)-1,np.exp(y_test)-1)\n",
    "    y_pred_test=model.predict(test)\n",
    "    return y_pred_train,rmsle_result,y_pred_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# A host of Scikit-learn models\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from catboost import CatBoostRegressor\n",
    "SEED=1\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    #nb = GaussianNB()\n",
    "    svc = SVR(C=100)\n",
    "    knn = KNeighborsRegressor(n_neighbors=3)\n",
    "    nn = MLPRegressor((80, 10), early_stopping=False, random_state=SEED)\n",
    "    gb = GradientBoostingRegressor(n_estimators=100, random_state=SEED)\n",
    "    rf = RandomForestRegressor(n_estimators=250, max_features='sqrt', max_depth=100, random_state=SEED)\n",
    "    cb= CatBoostRegressor(depth=4,iterations=1200,learning_rate=0.03,verbose=False)\n",
    "\n",
    "    models = {'svm': svc,\n",
    "              'knn': knn,\n",
    "              'mlp-nn': nn,\n",
    "              'random forest': rf,\n",
    "              'gbm': gb,\n",
    "              'catboost':cb\n",
    "              }\n",
    "\n",
    "    return models\n",
    "\n",
    "base_learners = get_models()\n",
    "\n",
    "\n",
    "#meta_learner = GradientBoostingRegressor(\n",
    "#    n_estimators=1000,\n",
    "#    loss=\"exponential\",\n",
    "#    max_features=4,\n",
    "#    max_depth=3,\n",
    "#    subsample=0.5,\n",
    "#    learning_rate=0.005, \n",
    "#    random_state=SEED\n",
    "#)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "(2400, 21) (2400,)\n",
      "\n",
      "Fitting 2 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/mlens/externals/sklearn/base.py:116: DeprecationWarning: Estimator CatBoostRegressor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/mlens/externals/sklearn/base.py:116: DeprecationWarning: Estimator CatBoostRegressor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:00:48\n",
      "\n",
      "Predicting 2 layers\n",
      "Predict complete                    | 00:00:00\n",
      "0.028519001794062512\n",
      "Eg\n",
      "(2400, 21) (2400,)\n",
      "\n",
      "Fitting 2 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/scratch/baruselli/inst/intelpython35/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:00:39\n",
      "\n",
      "Predicting 2 layers\n",
      "Predict complete                    | 00:00:00\n",
      "0.07582528911912395\n"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "import mlens\n",
    "from mlens.model_selection import Evaluator\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.metrics import rmse\n",
    "\n",
    "# Instantiate the ensemble with 10 folds\n",
    "meta_learner=CatBoostRegressor(iterations=1200,\n",
    "                            learning_rate=0.03,\n",
    "                            depth=4,\n",
    "                            loss_function='RMSE',\n",
    "                            eval_metric='RMSE',\n",
    "                            random_seed=SEED,\n",
    "                            od_type='Iter',\n",
    "                            od_wait=50,verbose=False)\n",
    "\n",
    "sl1 = SuperLearner(\n",
    "    folds=5,\n",
    "    verbose=True,\n",
    "    random_state=SEED,\n",
    "    scorer=mlens.metrics.rmse\n",
    ")\n",
    "sl2 = SuperLearner(\n",
    "    folds=5,\n",
    "    verbose=True,\n",
    "    random_state=SEED,\n",
    "    scorer=mlens.metrics.rmse\n",
    ")\n",
    "\n",
    "# Add the base learners and the meta learner\n",
    "sl1.add(learners1) \n",
    "sl1.add_meta(meta_learner)\n",
    "sl2.add(learners2) \n",
    "sl2.add_meta(meta_learner)\n",
    "\n",
    "\n",
    "sls=[sl1,sl2]\n",
    "#evaluator\n",
    "#evl = Evaluator(make_scorer(mlens.metrics.rmse), cv=5, shuffle=False)\n",
    "\n",
    "for i,y in enumerate(y_col):\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    y_test=df_total_test[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    \n",
    "    #evl.fit(X_train, y_train, sl, {}, n_iter=1)\n",
    "\n",
    "    # Train the ensemble\n",
    "    sls[i].fit(X_train, y_train)\n",
    "    preds = sls[i].predict(X_train)\n",
    "    print(rmse(y_train, preds))\n",
    "#    results.append(mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "#                          evl.summary['test_score_mean']['superlearner'],\n",
    "#                          evl.summary['test_score_std']['superlearner'],\n",
    "#                          mlens.metrics.rmse(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "#    print_scores(scores_df, 'mlens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "E SuperLearner(array_check=2, backend=None, folds=5,\n",
      "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
      "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
      "   random_state=235, shuffle=False,\n",
      "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
      "   indexer=FoldIndex(X=None, folds=5, raise_on_exc...b199d8>)],\n",
      "   n_jobs=-1, name='group-33', raise_on_exception=True, transformers=[])],\n",
      "   verbose=0)],\n",
      "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
      "       random_state=1, sample_size=20,\n",
      "       scorer=<function rmse at 0x7fd5c5b199d8>, shuffle=False,\n",
      "       verbose=True)\n",
      "shapes: (2400, 21) (2400,)\n",
      "\n",
      "Predicting 2 layers\n",
      "Predict complete                    | 00:00:00\n",
      "Eg SuperLearner(array_check=2, backend=None, folds=5,\n",
      "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
      "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
      "   random_state=235, shuffle=False,\n",
      "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
      "   indexer=FoldIndex(X=None, folds=5, raise_on_exc...b199d8>)],\n",
      "   n_jobs=-1, name='group-35', raise_on_exception=True, transformers=[])],\n",
      "   verbose=0)],\n",
      "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
      "       random_state=1, sample_size=20,\n",
      "       scorer=<function rmse at 0x7fd5c5b199d8>, shuffle=False,\n",
      "       verbose=True)\n",
      "shapes: (2400, 21) (2400,)\n",
      "\n",
      "Predicting 2 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../kaggle_varie/varie.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df_test[y_]=y_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict complete                    | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../kaggle_varie/varie.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df_test[y_]=y_pred\n"
     ]
    }
   ],
   "source": [
    "#write to csv\n",
    "%load_ext autoreload\n",
    "%aimport varie\n",
    "%autoreload 2\n",
    "#I use a different model for E and Eg\n",
    "varie.make_csv2(df_total_train_eval,pd.DataFrame(),df_total_test,\n",
    "#         (ensemble.RandomForestRegressor(max_depth= 11, max_features='log2', n_estimators= 55),\n",
    "#          ensemble.RandomForestRegressor(max_depth= 9, max_features='sqrt', n_estimators= 220)),\n",
    "            (sl1,sl2),\n",
    "         y_col,'sl2.csv',drop=drop_col,columns=['id','E','Eg'],\n",
    "         new_column_names=['id','formation_energy_ev_natom' ,'bandgap_energy_ev'],change_col_names=True,fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble = SuperLearner(folds=5, scorer=mse)\n",
    "#ensemble.add([xgb.XGBRegressor(), lgb.LGBMRegressor(n_estimators=30)])\n",
    "#ensemble.add_meta([LinearRegression()])\n",
    "\n",
    "evl = Evaluator(make_scorer(mlens.metrics.rmse), cv=5, shuffle=False)\n",
    "evl.fit(X_train, y_train, sl, {}, n_iter=1)\n",
    "\n",
    "\n",
    "scores_df['mlens'] = [mlens.metrics.rmse(y_train, ensemble.predict(X_train)),\n",
    "                      evl.summary['test_score_mean']['superlearner'],\n",
    "                      evl.summary['test_score_std']['superlearner'],\n",
    "                      mlens.metrics.rmse(y_test, ensemble.predict(X_test))]\n",
    "\n",
    "print_scores(scores_df, 'mlens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "evaluator.fit(X_train, y_train,sl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for y in y_col:\n",
    "    print(y)\n",
    "    y_train=df_total_train_eval[y].values\n",
    "    y_test=df_total_test[y].values\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    \n",
    "\n",
    "    model=CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE',verbose=False)\n",
    "    params= {\"depth\": scipy.stats.randint(1,5), \n",
    "             'iterations': scipy.stats.randint(1000,2000),\n",
    "             'learning_rate':lognuniform(low=-2,high=-1,base=10,size=100)}\n",
    "    \n",
    "    \n",
    "    grid=RandomizedSearchCV(sl,param_distributions=params, n_iter=20,cv=4,verbose=2,scoring=\"neg_mean_squared_error\" )\n",
    "\n",
    "                        \n",
    "    grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
